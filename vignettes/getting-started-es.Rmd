---
title: "Primeros pasos con metasurvey (ES)"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Primeros pasos con metasurvey (ES)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
link-citations: true
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE
)
```

## Introduccion

Trabajar con microdatos de encuestas de hogares implica una gran cantidad de
procesamiento repetitivo: recodificar variables categoricas, construir indicadores,
unir datos externos y calcular estimaciones ponderadas. Cada investigador escribe
su propia version de estas transformaciones, y el codigo rara vez se comparte o
documenta de forma que otros puedan reutilizarlo.

**metasurvey** aborda este problema proporcionando una capa de metaprogramacion
sobre el paquete `survey` [@lumley2004]. En lugar de escribir scripts ad hoc,
se construye un **pipeline** de transformaciones que es:

- **Documentado** -- cada paso lleva un comentario, sus variables de
  entrada/salida y sus dependencias
- **Reproducible** -- el pipeline puede guardarse como una receta y aplicarse
  a nuevos datos
- **Compartible** -- las recetas pueden publicarse en una API publica donde
  otros investigadores pueden descubrirlas y reutilizarlas

El pipeline tiene tres niveles:

1. **Steps** -- transformaciones individuales (compute, recode, rename,
   remove, join)
2. **Recipes** -- colecciones reutilizables de steps agrupados con metadatos
3. **Workflows** -- estimaciones estadisticas (`svymean`, `svytotal`,
   `svyby`) que producen las tablas finales

El paquete maneja el survey design ---estratificacion, conglomerados,
pesos replicados--- automaticamente a traves del objeto `Survey`. El usuario
se enfoca en el analisis sustantivo; metasurvey se encarga de la infraestructura.

## Instalacion

```{r install, eval = FALSE}
# Install from GitHub
# devtools::install_github("metaSurveyR/metasurvey")

library(metasurvey)
library(data.table)
```

## Creacion de un objeto Survey

Un objeto `Survey` agrupa los microdatos con metadatos sobre pesos, edicion
y tipo de encuesta. Comenzaremos con un ejemplo sencillo usando datos simulados
que imitan la estructura de la Encuesta Continua de Hogares (ECH) de Uruguay.

La ECH es una encuesta de hogares de panel rotativo realizada por el Instituto
Nacional de Estadistica (INE) de Uruguay. Las variables clave incluyen:

- **e27**: Edad en anios
- **e26**: Sexo (1 = Hombre, 2 = Mujer)
- **e51**: Nivel educativo (escala codificada 1-14)
- **POBPCOAC**: Condicion de actividad
  - 2 = Ocupado
  - 3-5 = Desocupado
  - 6-8 = Inactivo
- **ht11**: Ingreso del hogar
- **pesoano**: Factor de expansion anual (peso)

```{r create-survey}
library(metasurvey)
library(data.table)

set.seed(42)
n <- 200

# Simulate ECH-like microdata
dt <- data.table(
  numero = 1:n, # Household ID
  e27 = sample(18:80, n, replace = TRUE), # Age
  e26 = sample(c(1, 2), n, replace = TRUE), # Sex
  ht11 = round(runif(n, 5000, 80000)), # Household income (Uruguayan pesos)
  POBPCOAC = sample(c(2, 3, 4, 5, 6), n,
    replace = TRUE,
    prob = c(0.55, 0.03, 0.02, 0.03, 0.37)
  ), # Labor status
  dpto = sample(1:19, n, replace = TRUE), # Department
  pesoano = round(runif(n, 0.5, 3.0), 4) # Annual weight
)

# Create Survey object
svy <- Survey$new(
  data    = dt,
  edition = "2023",
  type    = "ech",
  psu     = NULL, # No PSU for simple random sample
  engine  = "data.table", # Fast data manipulation
  weight  = add_weight(annual = "pesoano")
)
```

La funcion `add_weight()` mapea etiquetas de periodicidad (por ejemplo, "annual",
"monthly") a los nombres de las columnas de pesos en los datos. Esto permite que
la misma receta funcione con distintas ediciones de la encuesta.

Es posible inspeccionar los datos en cualquier momento:

```{r inspect}
head(get_data(svy), 3)
```

## Trabajo con Steps

Los steps son **lazy por defecto**: se registran pero no se ejecutan hasta que
se llama a `bake_steps()`. Esto permite:

1. Construir un pipeline de transformacion completo
2. Inspeccionar y validar los steps antes de la ejecucion
3. Reutilizar secuencias de steps como recetas
4. Asegurar que todas las dependencias existan antes del procesamiento

### Calculo de nuevas variables

Se utiliza `step_compute()` para crear variables derivadas. El paquete automaticamente:

- Valida que las variables de entrada existan
- Detecta dependencias entre steps
- Optimiza las expresiones para mejorar el rendimiento

```{r step-compute}
svy <- step_compute(svy,
  # Convert income to thousands for readability
  ht11_thousands = ht11 / 1000,

  # Create employment indicator following ILO definitions
  employed = ifelse(POBPCOAC == 2, 1, 0),

  # Working age population (14+ years, ECH standard)
  working_age = ifelse(e27 >= 14, 1, 0),
  comment = "Basic labor force indicators"
)
```

Es posible agrupar calculos usando el parametro `.by` (similar a `data.table`):

```{r step-compute-grouped}
# Calculate mean household income per department
svy <- step_compute(svy,
  mean_income_dept = mean(ht11, na.rm = TRUE),
  .by = "dpto",
  comment = "Department-level income averages"
)
```

### Recodificacion en categorias

Se utiliza `step_recode()` para crear variables categoricas a partir de condiciones.
Las condiciones se evaluan **de arriba hacia abajo**, y la primera coincidencia es
la que se aplica.

```{r step-recode}
# Recode labor force status (POBPCOAC) into meaningful categories
svy <- step_recode(svy, labor_status,
  POBPCOAC == 2 ~ "Employed", # Ocupado
  POBPCOAC %in% 3:5 ~ "Unemployed", # Desocupado
  POBPCOAC %in% 6:8 ~ "Inactive", # Inactivo
  .default = "Not classified",
  comment = "Labor force status - ILO standard"
)

# Create standard age groups for labor statistics
svy <- step_recode(svy, age_group,
  e27 < 25 ~ "Youth (14-24)",
  e27 < 45 ~ "Adult (25-44)",
  e27 < 65 ~ "Mature (45-64)",
  .default = "Elderly (65+)",
  .to_factor = TRUE, # Convert to factor
  ordered = TRUE, # Ordered factor
  comment = "Age groups for labor analysis"
)

# Recode sex into descriptive labels
svy <- step_recode(svy, gender,
  e26 == 1 ~ "Male",
  e26 == 2 ~ "Female",
  .default = "Other",
  comment = "Gender classification"
)
```

### Renombrar y eliminar variables

Se renombran variables para mayor claridad o consistencia:

```{r step-rename}
svy <- step_rename(svy,
  age = e27, # Rename e27 to age
  sex_code = e26 # Keep original as sex_code
)
```

Se eliminan variables que ya no son necesarias:

```{r step-remove}
# Remove intermediate calculations
svy <- step_remove(svy, working_age, mean_income_dept)
```

### Union con datos externos

Se utiliza `step_join()` para unir datos de referencia externos. Esto es util para agregar:

- Nombres y clasificaciones geograficas
- Tipos de cambio o deflactores
- Benchmarks o metas externas

```{r step-join}
# Department names and regions
department_info <- data.table(
  dpto = 1:19,
  dpto_name = c(
    "Montevideo", "Artigas", "Canelones", "Cerro Largo",
    "Colonia", "Durazno", "Flores", "Florida", "Lavalleja",
    "Maldonado", "Paysandú", "Río Negro", "Rivera", "Rocha",
    "Salto", "San José", "Soriano", "Tacuarembó", "Treinta y Tres"
  ),
  region = c("Montevideo", rep("Interior", 18))
)

svy <- step_join(svy,
  department_info,
  by = "dpto",
  type = "left",
  comment = "Add department names and regions"
)
```

## Ejecucion de transformaciones

### Aplicacion de steps (bake)

Se llama a `bake_steps()` para ejecutar todas las transformaciones pendientes:

```{r bake, eval=FALSE}
svy <- bake_steps(svy)
head(get_data(svy), 3)
```

El historial de steps se preserva para documentacion y reproducibilidad:

```{r get-steps, eval=FALSE}
steps <- get_steps(svy)
length(steps) # Number of transformation steps

# View step details
cat("Step 1:", steps[[1]]$name, "\n")
cat("Comment:", steps[[1]]$comments, "\n")
```

### Visualizacion del pipeline

Es posible visualizar el pipeline de transformacion como un grafo dirigido:

```{r view-graph, eval = FALSE}
view_graph(svy, init_step = "Load ECH 2023")
```

Esto genera un grafo interactivo que muestra:

- Fuentes de datos y uniones
- Steps de transformacion
- Dependencias entre variables
- Comentarios y metadatos

## Estimaciones estadisticas

Una vez preparados los datos, se utiliza `workflow()` para calcular estimaciones
de encuesta. La funcion envuelve los estimadores del paquete `survey` [@lumley2004]
y devuelve resultados ordenados con errores estandar y coeficientes de variacion.

**Importante:** Se debe pasar el objeto survey dentro de un `list()`.

### Estimaciones basicas

```{r workflow-mean}
# Estimate mean household income
result <- workflow(
  list(svy),
  survey::svymean(~ht11, na.rm = TRUE),
  estimation_type = "annual"
)

result
```

La salida incluye:

- `estimate`: Estimacion puntual
- `se`: Error estandar
- `cv`: Coeficiente de variacion
- `var_name`: Nombre de la variable
- `level`: Nivel del factor (para variables categoricas)

### Estimaciones multiples

Es posible calcular varias estadisticas en una sola llamada:

```{r workflow-multi}
results <- workflow(
  list(svy),
  survey::svymean(~ht11, na.rm = TRUE), # Mean income
  survey::svytotal(~employed, na.rm = TRUE), # Total employed
  survey::svymean(~labor_status, na.rm = TRUE), # Employment distribution
  estimation_type = "annual"
)

results
```

### Estimacion por dominios

Se calculan estimaciones para subpoblaciones usando `survey::svyby()`:

```{r workflow-domain}
# Mean income by gender
income_by_gender <- workflow(
  list(svy),
  survey::svyby(~ht11, ~gender, survey::svymean, na.rm = TRUE),
  estimation_type = "annual"
)

income_by_gender
```

## Evaluacion de calidad

El **coeficiente de variacion (CV)** mide la confiabilidad de las estimaciones.
Un CV mas bajo indica estimaciones mas precisas. Siguiendo las pautas del
INE Uruguay [@ine2023]:

| Rango de CV | Categoria de calidad | Recomendacion |
|-------------|----------------------|---------------|
| < 5%        | Excelente            | Usar sin restricciones |
| 5%--10%     | Muy buena            | Usar con confianza |
| 10%--15%    | Buena                | Usar para la mayoria de los propositos |
| 15%--25%    | Aceptable            | Usar con precaucion, senialando limitaciones |
| 25%--35%    | Pobre                | Usar solo para tendencias generales |
| >= 35%      | No confiable         | No publicar |

Se utiliza `evaluate_cv()` para clasificar la calidad de las estimaciones:

```{r cv}
# Check quality of mean income estimate
cv_percentage <- results$cv[1] * 100
quality <- evaluate_cv(cv_percentage)

cat("CV:", round(cv_percentage, 2), "%\n")
cat("Quality:", quality, "\n")
```

Para estadisticas oficiales, siempre se debe reportar:

1. Estimacion puntual
2. Error estandar o intervalo de confianza
3. Coeficiente de variacion
4. Clasificacion de calidad
5. Tamanio de la muestra

## Trabajo con Recipes

Las recipes agrupan steps de transformacion para **reproducibilidad** y
**comparticion**. Una vez desarrollado un pipeline funcional, se puede convertir
en una receta que puede ser:

- Aplicada a diferentes ediciones de la encuesta
- Compartida con colaboradores
- Publicada para garantizar transparencia
- Versionada y documentada

### Creacion de una Recipe

Se crea una receta a partir de los steps desarrollados:

```{r recipe-create}
# Convert current steps to a recipe
labor_recipe <- steps_to_recipe(
  name = "ECH Labor Force Indicators",
  user = "National Statistics Office",
  svy = svy,
  description = paste(
    "Standard labor force indicators following ILO definitions.",
    "Creates employment status, age groups, and gender classifications."
  ),
  steps = get_steps(svy),
  topic = "labor_statistics"
)

class(labor_recipe)
labor_recipe$name
```

O se puede definir una receta desde cero:

```{r recipe-define, eval=FALSE}
minimal_recipe <- recipe(
  name = "Basic Demographics",
  user = "analyst",
  svy = survey_empty(type = "ech", edition = "2023"),
  description = "Basic demographic recoding",
  topic = "demographics",

  # Define steps inline
  step_recode(
    gender,
    e26 == 1 ~ "Male",
    e26 == 2 ~ "Female",
    .default = "Other"
  ),
  step_recode(
    age_group,
    e27 < 18 ~ "Minor",
    e27 < 65 ~ "Adult",
    .default = "Senior"
  )
)
```

### Aplicacion de Recipes a nuevos datos

Una vez publicada, cualquier persona puede recuperar una receta y aplicarla a sus datos:

```{r recipe-apply, eval = FALSE}
# Search for existing recipes
found <- search_recipes("labor")

# Apply to a new survey
new_svy <- add_recipe(new_svy, found[[1]])
processed <- bake_recipes(new_svy)
```

### Documentacion de Recipes

Las recetas documentan automaticamente sus transformaciones:

```{r recipe-doc}
doc <- labor_recipe$doc()
names(doc)

# Input variables required
doc$input_variables

# Output variables created
doc$output_variables
```

## Configuracion del paquete

metasurvey ofrece configuraciones globales que se pueden ajustar segun el flujo de trabajo:

```{r config}
# Check current lazy-processing setting
lazy_default() # TRUE = steps recorded but not executed immediately

# Check data-copy behavior
use_copy_default() # TRUE = operate on copies (safer but slower)

# View available computation engines
show_engines() # "data.table", "dplyr", etc.
```

Se pueden modificar las configuraciones para la sesion actual:

```{r config-set, eval = FALSE}
# Disable lazy evaluation (execute steps immediately)
set_lazy(FALSE)

# Modify inplace (faster, but modifies original data)
set_use_copy(FALSE)

# Reset to defaults
set_lazy(TRUE)
set_use_copy(TRUE)
```

## Compartir el trabajo: el ecosistema de recipes

Uno de los objetivos de metasurvey es reducir el esfuerzo duplicado en la
comunidad de investigadores que trabajan con encuestas. Si se ha construido un
pipeline de procesamiento util, es posible publicarlo para que otros lo encuentren
y reutilicen. El paquete se conecta a una API publica donde se almacenan recipes
y workflows:

```{r ecosystem, eval = FALSE}
# Browse existing recipes (no login required)
ech_recipes <- api_list_recipes(survey_type = "ech")
length(ech_recipes)

# Search for something specific
labor <- api_list_recipes(search = "labor")

# To publish your own, create an account first
api_register("Your Name", "you@example.com", "password")

# Then publish
api_publish_recipe(labor_recipe)
```

El ecosistema soporta tres niveles de certificacion (comunitaria, revisada,
oficial) y tres tipos de cuenta (individual, miembro institucional,
institucion). Las cuentas institucionales requieren aprobacion de un
administrador, lo que asegura que las certificaciones tengan respaldo real.

Para mas detalles, consultar [Creacion y comparticion de Recipes](recipes.html).

## Paquetes relacionados

metasurvey es parte de un ecosistema creciente de paquetes de R para el
analisis de encuestas de hogares en America Latina:

- **[ech](https://calcita.github.io/ech/)** -- Funciones para el calculo de
  indicadores socioeconomicos con la ECH de Uruguay. Provee funciones listas
  para usar sobre pobreza, ingresos, educacion e indicadores de empleo.
- **[eph](https://docs.ropensci.org/eph/)** -- Herramientas para trabajar con
  la Encuesta Permanente de Hogares de Argentina. Publicado en rOpenSci.
  Cubre descarga de datos, construccion de paneles y calculo de pobreza.
- **[survey](https://cran.r-project.org/package=survey)** -- El paquete
  fundamental para inferencia basada en disenio con encuestas complejas
  [@lumley2004]. metasurvey se construye sobre el.

## Proximos pasos

Ahora que se comprenden los conceptos basicos, se pueden explorar estas guias:

- **[Creacion y comparticion de Recipes](recipes.html)** -- Registro de recipes, certificacion y descubrimiento
- **[Workflows de estimacion](workflows-and-estimation.html)** -- `workflow()`, `RecipeWorkflow` y estimaciones publicables
- **[Survey designs y validacion](complex-designs.html)** -- Estratificacion, conglomerados, pesos replicados, validacion del pipeline
- **[Paneles rotativos y PoolSurvey](panel-analysis.html)** -- Analisis longitudinal con `RotativePanelSurvey` y `PoolSurvey`
- **[Caso de estudio ECH](ech-case-study.html)** -- Analisis completo del mercado laboral con comparacion con STATA

## Referencias
