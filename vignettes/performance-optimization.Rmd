---
title: "Performance Optimization and Big Data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Performance Optimization and Big Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE,
  eval = requireNamespace("future", quietly = TRUE) &&
    requireNamespace("future.apply", quietly = TRUE) &&
    requireNamespace("magrittr", quietly = TRUE)
)
```

```{r setup}
library(metasurvey)
library(survey)
library(magrittr)
library(data.table)
library(parallel)
library(future)
library(future.apply)
```

## Introduction

This vignette covers performance optimization techniques for working with large survey datasets using `metasurvey`. As survey datasets grow in size and complexity, efficient processing becomes crucial for practical analysis.

Topics covered:
- Memory-efficient data loading and processing
- Parallel processing for multiple surveys
- Optimizing workflow performance
- Handling very large datasets
- Caching and memoization strategies
- Database integration for large-scale operations

## Memory-Efficient Data Loading

### Chunked Data Processing

For datasets too large to fit in memory:

```{r chunked-processing}
# Process large survey files in chunks
process_large_survey <- function(file_path, chunk_size = 50000, 
                                recipes = NULL, estimation_calls = list()) {
  
  # Get file information without loading
  file_info <- data.table::fread(file_path, nrows = 0)
  total_rows <- data.table::fread(file_path, select = 1L, verbose = FALSE)[, .N]
  n_chunks <- ceiling(total_rows / chunk_size)
  
  message("Processing ", total_rows, " rows in ", n_chunks, " chunks")
  
  chunk_results <- vector("list", n_chunks)
  
  for (i in seq_len(n_chunks)) {
    skip_rows <- (i - 1) * chunk_size
    
    # Read chunk
    chunk_data <- data.table::fread(
      file_path,
      skip = skip_rows,
      nrows = chunk_size,
      verbose = FALSE
    )
    
    if (nrow(chunk_data) == 0) break
    
    message("Processing chunk ", i, "/", n_chunks, " (", nrow(chunk_data), " rows)")
    
    # Process chunk
    chunk_survey <- load_survey(
      data = chunk_data,
      svy_edition = "chunk",
      svy_type = "large_survey",
      svy_weight = add_weight(main = "weight")
    )
    
    # Apply recipes if provided
    if (!is.null(recipes)) {
      for (recipe in recipes) {
        chunk_survey <- chunk_survey %>% add_recipe(recipe)
      }
      chunk_survey <- bake_recipes(chunk_survey)
    }
    
    # Run estimations
    if (length(estimation_calls) > 0) {
      chunk_result <- do.call(workflow, c(
        list(survey = list(chunk_survey)),
        estimation_calls,
        list(estimation_type = "main")
      ))
      chunk_results[[i]] <- chunk_result
    }
    
    # Clean up memory
    rm(chunk_data, chunk_survey)
    gc()
  }
  
  # Combine results
  if (length(estimation_calls) > 0) {
    combined_results <- rbindlist(chunk_results, fill = TRUE)
    return(combined_results)
  }
  
  return(NULL)
}

# Example usage
# large_results <- process_large_survey(
#   "very_large_survey.csv",
#   chunk_size = 25000,
#   recipes = list(basic_recipe),
#   estimation_calls = list(
#     survey::svymean(~income),
#     survey::svytotal(~employed)
#   )
# )
```

### Streaming Data Processing

For continuous data streams or very large files:

```{r streaming}
# Streaming survey processor
create_streaming_processor <- function() {
  # Initialize accumulator state
  accumulator <- list(
    n_total = 0,
    sum_weights = 0,
    weighted_sums = list(),
    weighted_sum_squares = list()
  )
  
  # Process function for each data batch
  process_batch <- function(data_batch, weight_var = "weight") {
    # Update accumulator
    accumulator$n_total <<- accumulator$n_total + nrow(data_batch)
    accumulator$sum_weights <<- accumulator$sum_weights + sum(data_batch[[weight_var]], na.rm = TRUE)
    
    # Update weighted statistics for numeric variables
    numeric_vars <- names(data_batch)[sapply(data_batch, is.numeric)]
    numeric_vars <- setdiff(numeric_vars, weight_var)
    
    for (var in numeric_vars) {
      if (!(var %in% names(accumulator$weighted_sums))) {
        accumulator$weighted_sums[[var]] <<- 0
        accumulator$weighted_sum_squares[[var]] <<- 0
      }
      
      # Update weighted sums
      weights <- data_batch[[weight_var]]
      values <- data_batch[[var]]
      valid_idx <- !is.na(values) & !is.na(weights)
      
      accumulator$weighted_sums[[var]] <<- accumulator$weighted_sums[[var]] + 
        sum(weights[valid_idx] * values[valid_idx])
      
      accumulator$weighted_sum_squares[[var]] <<- accumulator$weighted_sum_squares[[var]] + 
        sum(weights[valid_idx] * values[valid_idx]^2)
    }
  }
  
  # Get final estimates
  get_estimates <- function() {
    estimates <- list()
    
    for (var in names(accumulator$weighted_sums)) {
      # Weighted mean
      estimates[[paste0(var, "_mean")]] <- accumulator$weighted_sums[[var]] / accumulator$sum_weights
      
      # Weighted variance (approximate)
      mean_sq <- accumulator$weighted_sum_squares[[var]] / accumulator$sum_weights
      sq_mean <- (accumulator$weighted_sums[[var]] / accumulator$sum_weights)^2
      estimates[[paste0(var, "_var")]] <- mean_sq - sq_mean
    }
    
    estimates$total_observations <- accumulator$n_total
    estimates$sum_weights <- accumulator$sum_weights
    
    return(estimates)
  }
  
  # Reset accumulator
  reset <- function() {
    accumulator <<- list(
      n_total = 0,
      sum_weights = 0,
      weighted_sums = list(),
      weighted_sum_squares = list()
    )
  }
  
  return(list(
    process_batch = process_batch,
    get_estimates = get_estimates,
    reset = reset
  ))
}

# Example usage
# processor <- create_streaming_processor()
# 
# # Process data in batches
# for (batch_file in batch_files) {
#   batch_data <- fread(batch_file)
#   processor$process_batch(batch_data, weight_var = "survey_weight")
# }
# 
# # Get final estimates
# final_estimates <- processor$get_estimates()
```

## Parallel Processing

### Survey-Level Parallelization

Process multiple surveys simultaneously:

```{r parallel-surveys}
# Set up parallel processing
setup_parallel_metasurvey <- function(n_cores = NULL) {
  if (is.null(n_cores)) {
    n_cores <- max(1, parallel::detectCores() - 1)
  }
  
  # Configure future backend
  future::plan(future::multisession, workers = n_cores)
  
  message("Configured parallel processing with ", n_cores, " cores")
  return(n_cores)
}

# Parallel survey processing function
process_surveys_parallel <- function(survey_paths, recipe_list = NULL, 
                                   estimation_calls = list(), n_cores = NULL) {
  
  # Setup parallel processing
  n_cores <- setup_parallel_metasurvey(n_cores)
  
  # Process surveys in parallel
  results <- future.apply::future_lapply(survey_paths, function(path) {
    tryCatch({
      # Load survey
      survey <- load_survey(
        data = path,
        svy_edition = basename(path),
        svy_type = "parallel_survey",
        svy_weight = add_weight(main = "weight")
      )
      
      # Apply recipes
      if (!is.null(recipe_list)) {
        for (recipe in recipe_list) {
          survey <- survey %>% add_recipe(recipe)
        }
        survey <- bake_recipes(survey)
      }
      
      # Run estimations
      if (length(estimation_calls) > 0) {
        result <- do.call(workflow, c(
          list(survey = list(survey)),
          estimation_calls,
          list(estimation_type = "main")
        ))
        
        # Add survey identifier
        result[, survey_file := basename(path)]
        return(result)
      }
      
      return(data.table(survey_file = basename(path), status = "processed"))
      
    }, error = function(e) {
      return(data.table(
        survey_file = basename(path),
        status = "error",
        error_message = e$message
      ))
    })
  }, future.seed = TRUE)
  
  # Combine results
  combined_results <- rbindlist(results, fill = TRUE)
  
  # Clean up parallel backend
  future::plan(future::sequential)
  
  return(combined_results)
}

# Example usage for time series
# monthly_files <- list.files("monthly_surveys/", pattern = "*.csv", full.names = TRUE)
# 
# parallel_results <- process_surveys_parallel(
#   monthly_files,
#   recipe_list = list(labor_force_recipe),
#   estimation_calls = list(
#     survey::svymean(~unemployment_rate),
#     survey::svytotal(~employed)
#   ),
#   n_cores = 4
# )
```

### Bootstrap Parallel Processing

Parallelize bootstrap variance estimation:

```{r parallel-bootstrap}
# Parallel bootstrap processing
parallel_bootstrap_workflow <- function(survey, estimation_calls, n_cores = NULL) {
  
  if (is.null(n_cores)) {
    n_cores <- max(1, parallel::detectCores() - 1)
  }
  
  # Get bootstrap replicate weights
  design <- survey$design$main
  
  if (!"survey.bootstrap" %in% class(design)) {
    stop("Survey must have bootstrap replicate weights for parallel processing")
  }
  
  replicate_weights <- design$repweights
  n_replicates <- ncol(replicate_weights)
  
  message("Processing ", n_replicates, " bootstrap replicates in parallel")
  
  # Setup parallel cluster
  cl <- parallel::makeCluster(n_cores)
  on.exit(parallel::stopCluster(cl))
  
  # Load packages on workers
  parallel::clusterEvalQ(cl, {
    library(metasurvey)
    library(survey)
    library(data.table)
  })
  
  # Export objects to workers
  parallel::clusterExport(cl, c("survey", "estimation_calls"), envir = environment())
  
  # Process replicates in parallel
  replicate_results <- parallel::parLapply(cl, 1:n_replicates, function(i) {
    tryCatch({
      # Create design with single replicate
      rep_design <- survey$design$main
      rep_design$prob <- 1 / replicate_weights[, i]
      
      # Run estimations on this replicate
      rep_results <- list()
      for (j in seq_along(estimation_calls)) {
        call_result <- eval(estimation_calls[[j]], envir = list(design = rep_design))
        rep_results[[j]] <- as.numeric(call_result)
      }
      
      return(rep_results)
      
    }, error = function(e) {
      return(NA)
    })
  })
  
  # Process results
  valid_results <- replicate_results[!is.na(replicate_results)]
  
  if (length(valid_results) < n_replicates * 0.8) {
    warning("Less than 80% of bootstrap replicates succeeded")
  }
  
  # Calculate variance estimates
  replicate_matrix <- do.call(rbind, valid_results)
  
  # Main estimates (using original weights)
  main_results <- do.call(workflow, c(
    list(survey = list(survey)),
    estimation_calls,
    list(estimation_type = "main")
  ))
  
  # Bootstrap variance estimates
  bootstrap_vars <- apply(replicate_matrix, 2, var, na.rm = TRUE)
  
  # Combine with main results
  main_results[, bootstrap_se := sqrt(bootstrap_vars)]
  main_results[, bootstrap_cv := bootstrap_se / abs(value)]
  
  return(main_results)
}
```

## Workflow Optimization

### Efficient Step Processing

Optimize step execution for large datasets:

```{r efficient-steps}
# Optimized step processing
optimize_step_processing <- function() {
  
  # Vectorized operations for common patterns
  fast_recode <- function(data, var_name, ..., .default = NA) {
    conditions <- list(...)
    result <- rep(.default, nrow(data))
    
    for (i in seq_along(conditions)) {
      condition_expr <- conditions[[i]]
      # Use data.table's efficient evaluation
      if (is(data, "data.table")) {
        mask <- data[, eval(condition_expr[[2]])]
      } else {
        mask <- eval(condition_expr[[2]], envir = data)
      }
      result[mask] <- condition_expr[[3]]
    }
    
    return(result)
  }
  
  # Batch variable creation
  batch_compute <- function(data, expressions) {
    if (is(data, "data.table")) {
      # Use data.table's efficient := operator
      for (var_name in names(expressions)) {
        data[, (var_name) := eval(expressions[[var_name]])]
      }
    } else {
      # Fallback for data.frame
      for (var_name in names(expressions)) {
        data[[var_name]] <- eval(expressions[[var_name]], envir = data)
      }
    }
    return(data)
  }
  
  return(list(
    fast_recode = fast_recode,
    batch_compute = batch_compute
  ))
}

# Optimized recipe application
apply_recipe_optimized <- function(survey, recipe) {
  optimizers <- optimize_step_processing()
  
  # Group steps by type for batch processing
  compute_steps <- list()
  recode_steps <- list()
  
  for (step in recipe$steps) {
    if (step$type == "compute") {
      compute_steps <- c(compute_steps, step$exprs)
    } else if (step$type == "recode") {
      recode_steps[[step$new_var]] <- step
    }
  }
  
  # Batch process compute steps
  if (length(compute_steps) > 0) {
    survey$data <- optimizers$batch_compute(survey$data, compute_steps)
  }
  
  # Process recode steps efficiently
  for (var_name in names(recode_steps)) {
    step <- recode_steps[[var_name]]
    survey$data[[var_name]] <- optimizers$fast_recode(
      survey$data, var_name, step$exprs, .default = step$.default
    )
  }
  
  return(survey)
}
```

### Caching and Memoization

Implement intelligent caching for repeated operations:

```{r caching-system}
# Advanced caching system
create_metasurvey_cache <- function(max_size_mb = 1000) {
  cache_env <- new.env(parent = emptyenv())
  cache_info <- list(
    size_mb = 0,
    max_size_mb = max_size_mb,
    access_times = list(),
    sizes_mb = list()
  )
  
  # Hash function for cache keys
  create_cache_key <- function(...) {
    key_components <- list(...)
    digest::digest(key_components, algo = "xxhash64")
  }
  
  # Get object size in MB
  get_size_mb <- function(obj) {
    as.numeric(object.size(obj)) / (1024^2)
  }
  
  # Cache eviction (LRU)
  evict_if_needed <- function(new_size_mb) {
    while (cache_info$size_mb + new_size_mb > cache_info$max_size_mb && 
           length(cache_info$access_times) > 0) {
      
      # Find least recently used item
      oldest_key <- names(cache_info$access_times)[
        which.min(unlist(cache_info$access_times))
      ]
      
      # Remove from cache
      if (exists(oldest_key, envir = cache_env)) {
        rm(list = oldest_key, envir = cache_env)
        cache_info$size_mb <<- cache_info$size_mb - cache_info$sizes_mb[[oldest_key]]
        cache_info$access_times[[oldest_key]] <<- NULL
        cache_info$sizes_mb[[oldest_key]] <<- NULL
      }
    }
  }
  
  # Store in cache
  put <- function(key, value) {
    size_mb <- get_size_mb(value)
    evict_if_needed(size_mb)
    
    assign(key, value, envir = cache_env)
    cache_info$size_mb <<- cache_info$size_mb + size_mb
    cache_info$access_times[[key]] <<- Sys.time()
    cache_info$sizes_mb[[key]] <<- size_mb
  }
  
  # Retrieve from cache
  get <- function(key) {
    if (exists(key, envir = cache_env)) {
      cache_info$access_times[[key]] <<- Sys.time()
      return(get(key, envir = cache_env))
    }
    return(NULL)
  }
  
  # Check if key exists
  exists_key <- function(key) {
    exists(key, envir = cache_env)
  }
  
  # Clear cache
  clear <- function() {
    rm(list = ls(envir = cache_env), envir = cache_env)
    cache_info$size_mb <<- 0
    cache_info$access_times <<- list()
    cache_info$sizes_mb <<- list()
  }
  
  # Cache statistics
  stats <- function() {
    list(
      items = length(ls(envir = cache_env)),
      size_mb = round(cache_info$size_mb, 2),
      max_size_mb = cache_info$max_size_mb,
      utilization = round(cache_info$size_mb / cache_info$max_size_mb * 100, 1)
    )
  }
  
  return(list(
    create_key = create_cache_key,
    put = put,
    get = get,
    exists = exists_key,
    clear = clear,
    stats = stats
  ))
}

# Cached workflow function
cached_workflow <- function(cache, survey, ..., cache_key = NULL) {
  # Generate cache key if not provided
  if (is.null(cache_key)) {
    cache_key <- cache$create_key(
      survey_hash = digest::digest(survey$data),
      survey_metadata = list(survey$type, survey$edition, survey$weight),
      calls = list(...)
    )
  }
  
  # Check cache first
  if (cache$exists(cache_key)) {
    message("Using cached result")
    return(cache$get(cache_key))
  }
  
  # Compute result
  result <- workflow(survey = list(survey), ..., estimation_type = "main")
  
  # Store in cache
  cache$put(cache_key, result)
  
  return(result)
}

# Example usage
# cache <- create_metasurvey_cache(max_size_mb = 500)
# 
# # First call - computed and cached
# result1 <- cached_workflow(
#   cache, survey_data,
#   survey::svymean(~income),
#   cache_key = "income_analysis_2023"
# )
# 
# # Second call - retrieved from cache
# result2 <- cached_workflow(
#   cache, survey_data,
#   survey::svymean(~income),
#   cache_key = "income_analysis_2023"
# )
# 
# # Check cache stats
# print(cache$stats())
```

## Database Integration

### Working with Database-Stored Surveys

For very large surveys stored in databases:

```{r database-integration}
# Database-backed survey processing
create_db_survey_processor <- function(db_connection, table_name) {
  
  # Lazy evaluation wrapper
  process_db_survey <- function(where_clause = NULL, sample_frac = NULL, 
                               recipes = NULL, estimation_calls = list()) {
    
    # Build query
    query <- paste("SELECT * FROM", table_name)
    
    if (!is.null(where_clause)) {
      query <- paste(query, "WHERE", where_clause)
    }
    
    if (!is.null(sample_frac)) {
      query <- paste(query, "ORDER BY RANDOM() LIMIT", 
                    paste0("(SELECT COUNT(*) * ", sample_frac, " FROM ", table_name, ")"))
    }
    
    # Execute query and load data
    message("Executing query: ", query)
    survey_data <- DBI::dbGetQuery(db_connection, query)
    
    # Convert to data.table for efficiency
    data.table::setDT(survey_data)
    
    # Create survey object
    survey <- load_survey(
      data = survey_data,
      svy_edition = "db_extract",
      svy_type = "database_survey",
      svy_weight = add_weight(main = "weight")
    )
    
    # Apply recipes
    if (!is.null(recipes)) {
      for (recipe in recipes) {
        survey <- survey %>% add_recipe(recipe)
      }
      survey <- bake_recipes(survey)
    }
    
    # Run estimations
    if (length(estimation_calls) > 0) {
      results <- do.call(workflow, c(
        list(survey = list(survey)),
        estimation_calls,
        list(estimation_type = "main")
      ))
      return(results)
    }
    
    return(survey)
  }
  
  # Database summary statistics
  get_db_summary <- function() {
    summary_query <- paste0("
      SELECT 
        COUNT(*) as total_rows,
        COUNT(DISTINCT weight) as unique_weights,
        MIN(weight) as min_weight,
        MAX(weight) as max_weight,
        AVG(weight) as avg_weight
      FROM ", table_name)
    
    summary <- DBI::dbGetQuery(db_connection, summary_query)
    return(summary)
  }
  
  return(list(
    process = process_db_survey,
    summary = get_db_summary
  ))
}

# Example usage with database
# library(DBI)
# library(RSQLite)
# 
# # Connect to database
# con <- dbConnect(RSQLite::SQLite(), "large_survey.db")
# 
# # Create processor
# db_processor <- create_db_survey_processor(con, "survey_2023")
# 
# # Get database summary
# db_summary <- db_processor$summary()
# print(db_summary)
# 
# # Process subset of data
# subset_results <- db_processor$process(
#   where_clause = "region = 'North'",
#   recipes = list(labor_force_recipe),
#   estimation_calls = list(
#     survey::svymean(~unemployment_rate),
#     survey::svytotal(~employed)
#   )
# )
```

## Performance Monitoring

### Benchmarking and Profiling

Monitor and optimize performance:

```{r performance-monitoring}
# Performance monitoring utilities
create_performance_monitor <- function() {
  
  # Benchmark wrapper for functions
  benchmark_function <- function(func, ..., n_runs = 5, warmup = 1) {
    
    # Warmup runs
    for (i in seq_len(warmup)) {
      suppressMessages(func(...))
    }
    
    # Benchmark runs
    times <- numeric(n_runs)
    memory_usage <- numeric(n_runs)
    
    for (i in seq_len(n_runs)) {
      gc()  # Clean up before each run
      
      start_time <- Sys.time()
      start_memory <- as.numeric(object.size(ls(envir = .GlobalEnv)))
      
      result <- func(...)
      
      end_time <- Sys.time()
      end_memory <- as.numeric(object.size(ls(envir = .GlobalEnv)))
      
      times[i] <- as.numeric(difftime(end_time, start_time, units = "secs"))
      memory_usage[i] <- (end_memory - start_memory) / (1024^2)  # MB
    }
    
    return(list(
      times = times,
      mean_time = mean(times),
      sd_time = sd(times),
      memory_usage = memory_usage,
      mean_memory = mean(memory_usage)
    ))
  }
  
  # Profile workflow execution
  profile_workflow <- function(survey, ...) {
    
    # Profile using Rprof
    temp_prof <- tempfile(fileext = ".prof")
    
    Rprof(temp_prof, memory.profiling = TRUE)
    result <- workflow(survey = list(survey), ..., estimation_type = "main")
    Rprof(NULL)
    
    # Parse profiling results
    prof_summary <- summaryRprof(temp_prof, memory = "both")
    
    # Clean up
    unlink(temp_prof)
    
    return(list(
      result = result,
      timing = prof_summary$by.self,
      memory = prof_summary$by.self[, "mem.total", drop = FALSE]
    ))
  }
  
  # Memory usage tracking
  track_memory_usage <- function(func, ...) {
    gc()  # Clean up first
    
    initial_memory <- gc(verbose = FALSE)
    result <- func(...)
    final_memory <- gc(verbose = FALSE)
    
    memory_change <- final_memory[2, "used"] - initial_memory[2, "used"]
    
    return(list(
      result = result,
      memory_change_mb = memory_change,
      peak_memory_mb = final_memory[2, "max used"]
    ))
  }
  
  return(list(
    benchmark = benchmark_function,
    profile = profile_workflow,
    track_memory = track_memory_usage
  ))
}

# Performance optimization recommendations
optimize_metasurvey_performance <- function() {
  
  recommendations <- list()
  
  # Check data.table usage
  recommendations$data_table <- "Use data.table for large datasets - set data.table::setDTthreads() for parallel operations"
  
  # Memory recommendations
  total_memory_gb <- as.numeric(system("sysctl hw.memsize", intern = TRUE)) / (1024^3)
  recommendations$memory <- paste0(
    "Available memory: ", round(total_memory_gb, 1), "GB. ",
    "Recommended max dataset size: ", round(total_memory_gb * 0.3, 1), "GB"
  )
  
  # CPU recommendations
  n_cores <- parallel::detectCores()
  recommendations$cpu <- paste0(
    "Available CPU cores: ", n_cores, ". ",
    "Recommended parallel workers: ", max(1, n_cores - 1)
  )
  
  # Survey-specific recommendations
  recommendations$survey_processing <- c(
    "Use chunked processing for datasets > 1GB",
    "Apply recipes in batch rather than individually",
    "Cache intermediate results for repeated analysis",
    "Use bootstrap replicates over design-based variance when possible",
    "Process multiple surveys in parallel for time series analysis"
  )
  
  return(recommendations)
}

# Example usage
# monitor <- create_performance_monitor()
# 
# # Benchmark workflow execution
# benchmark_results <- monitor$benchmark(
#   workflow,
#   survey = list(large_survey),
#   survey::svymean(~income),
#   estimation_type = "main",
#   n_runs = 3
# )
# 
# print(benchmark_results)
# 
# # Get optimization recommendations
# recommendations <- optimize_metasurvey_performance()
# print(recommendations)
```

## Best Practices for Large-Scale Analysis

### 1. Data Management
```{r bp-data-management}
# Use appropriate data types to minimize memory usage
# Convert character variables to factors when appropriate
# Use data.table for large datasets (>100MB)
# Implement data compression for storage
```

### 2. Processing Strategy
```{r bp-processing}
# Process in logical chunks based on available memory
# Use streaming for datasets that don't fit in memory
# Parallelize independent operations
# Cache intermediate results strategically
```

### 3. Memory Management
```{r bp-memory}
# Regular garbage collection in long-running processes
# Remove large objects when no longer needed
# Monitor memory usage throughout processing
# Use memory-mapped files for very large datasets
```

### 4. Quality vs Performance Trade-offs
```{r bp-tradeoffs}
# Balance sample size vs processing time for bootstrap replicates
# Consider approximate methods for exploratory analysis
# Use design-based variance for final publication estimates
# Implement early stopping for iterative procedures
```

### 5. Monitoring and Debugging
```{r bp-monitoring}
# Log processing times and memory usage
# Implement progress reporting for long operations
# Use profiling to identify bottlenecks
# Test scaling behavior with different data sizes
```

## Conclusion

Performance optimization in `metasurvey` enables:

- **Scalable analysis** of large survey datasets
- **Efficient use** of computational resources
- **Reduced processing time** through parallelization
- **Memory-efficient workflows** for limited-resource environments
- **Cached operations** for repeated analysis

Key strategies:
1. **Choose appropriate tools** for data size and complexity
2. **Parallelize independent operations** when possible
3. **Monitor resource usage** to identify bottlenecks  
4. **Cache results** intelligently to avoid recomputation
5. **Profile and benchmark** to guide optimization efforts

These techniques make `metasurvey` suitable for production environments and large-scale statistical operations while maintaining the package's focus on reproducibility and quality.
