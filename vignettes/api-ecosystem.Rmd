---
title: "The Recipe Ecosystem: Sharing and Discovering Survey Processing Pipelines"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{The Recipe Ecosystem: Sharing and Discovering Survey Processing Pipelines}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
link-citations: true
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE,
  eval = FALSE
)
```

## The problem we are trying to solve

Anyone who has worked with household survey microdata knows the feeling: you
download the raw files from the national statistics office, open the
codebook, and spend days writing code to reclassify labor status,
harmonize income variables, and build the indicators you actually
need. A few months later, a colleague at a different university starts
the exact same project---and writes the exact same code from scratch.

This happens constantly with Uruguay's Encuesta Continua de Hogares (ECH).
Every researcher who works with the ECH ends up writing their own version
of `POBPCOAC == 2 ~ "Employed"`. The definitions are standardized by the
INE and the ILO, yet the code to implement them is not. Errors creep in.
Results diverge. Time is wasted.

The **metasurvey recipe ecosystem** is our answer to this problem. It
provides a central place where researchers can:

- **Publish** their data processing pipelines as versioned, documented recipes
- **Discover** existing recipes before writing code from scratch
- **Reuse** community-tested pipelines with a single function call
- **Certify** recipes through institutional review for official use

In this guide we walk through the complete workflow, from creating an
account to publishing and discovering recipes through the public API.

## Setting up your account

The metasurvey package connects to a public API by default, so no
configuration is needed to browse recipes. To publish your own work, you
need an account.

```{r setup}
library(metasurvey)

# The API URL is configured automatically. You can check it:
getOption("metasurvey.api_url")
```

### Individual accounts

Individual researchers can create an account immediately:

```{r register-individual}
# Register as an individual researcher
api_register(
  name      = "Juan Perez",
  email     = "juan.perez@example.com",
  password  = "a_secure_password",
  user_type = "individual"
)
# You are now logged in and ready to publish.
```

### Institutional accounts

If you represent a research center, statistics office, or university
department, you can register as an institution. Institutional accounts
carry more weight in the certification system---recipes certified by an
institution like the Instituto de Economia (IECON) or the INE itself
signal a higher level of trust to other users.

Because of this, institutional accounts require admin approval before
activation:

```{r register-institution}
# Register an institution (requires admin review)
api_register(
  name      = "Instituto de Economia",
  email     = "iecon@fcea.edu.uy",
  password  = "institutional_password",
  user_type = "institution"
)
# Message: "Account created. Institutional accounts require admin approval."
# You will be able to login once the admin reviews your request.
```

Similarly, researchers who want to be affiliated with an institution
register as `institutional_member`:

```{r register-member}
api_register(
  name        = "Maria Garcia",
  email       = "maria@fcea.edu.uy",
  password    = "member_password",
  user_type   = "institutional_member",
  institution = "Instituto de Economia"
)
# This also requires admin approval.
```

This review process prevents unauthorized claims of institutional
affiliation. The admin receives a notification and can approve or reject
the request through the Shiny app or the API directly.

### Logging in

Once your account is approved (or immediately for individual accounts):

```{r login}
api_login("juan.perez@example.com", "a_secure_password")
# Message: "Logged in as: juan.perez@example.com"
```

The authentication token is stored automatically in your R session. It
refreshes transparently when it is close to expiring, so you do not need
to worry about session management during a working session.

You can also store credentials as environment variables for non-interactive
use (CI pipelines, automated reports):

```{r env-vars}
# In your .Renviron file:
# METASURVEY_API_URL=https://metasurvey-api-production.up.railway.app
# METASURVEY_TOKEN=eyJhbGciOiJIUzI1NiIs...
```

## Discovering existing recipes

Before writing any processing code, it is worth checking what the
community has already published. This is especially useful when starting
a new project or when a student joins a research group and needs to get
up to speed quickly.

### Browsing all available recipes

```{r list-all}
# See all recipes in the ecosystem
all_recipes <- api_list_recipes()
length(all_recipes)

# See just the ECH recipes
ech_recipes <- api_list_recipes(svy_type = "ech")
length(ech_recipes)
```

### Searching by topic

Suppose you are starting a project on labor market informality in
Uruguay. You want to know if someone has already coded the standard
definitions:

```{r search}
# Search by keyword
labor_recipes <- api_list_recipes(search = "labor")

# Inspect the first result
if (length(labor_recipes) > 0) {
  r <- labor_recipes[[1]]
  cat("Name:", r$name, "\n")
  cat("Author:", r$user, "\n")
  cat("Description:", r$description, "\n")
}
```

### Filtering by certification level

The certification system has three levels:

- **community**: Published by any registered user. No formal review.
- **reviewed**: Verified by an institutional member. The methodology has
  been checked against documentation.
- **official**: Certified by an institution (e.g., INE, IECON). Suitable
  for replication of official statistics.

```{r filter-certification}
# Only show officially certified recipes
official <- api_list_recipes(certification = "official")
```

### Using a recipe you found

Once you find a recipe that fits your needs, applying it to your data is
straightforward:

```{r use-recipe}
# Get a specific recipe by ID
recipe <- api_get_recipe("ech_labor_force_2023")

# Check what variables it needs
doc <- recipe$doc()
cat("Required input variables:\n")
cat(paste(" -", doc$input_variables), sep = "\n")

cat("\nVariables it creates:\n")
cat(paste(" -", doc$output_variables), sep = "\n")

# Validate against your survey data
validation <- recipe$validate(my_survey)
if (validation$validation_passed) {
  my_survey$add_recipe(recipe)
  my_survey <- bake_recipes(my_survey)
  cat("Recipe applied successfully.\n")
}
```

## Publishing your work

### A practical example: income quintiles for the ECH

Let's say you have developed a recipe that constructs per capita household
income quintiles for the ECH, accounting for regional price differences.
This is a common preprocessing step that many researchers need, and
getting it right requires attention to several details (deflation,
equivalence scales, regional adjustment).

First, build the recipe in your local session:

```{r create-recipe}
library(data.table)

# Start with simulated ECH-like data
set.seed(42)
n <- 500
dt <- data.table(
  numero   = 1:n,
  ht11     = round(runif(n, 5000, 120000)),
  ht19     = round(runif(n, 2000, 80000)),
  cant_per = sample(1:7, n, replace = TRUE),
  dpto     = sample(1:19, n, replace = TRUE),
  e27      = sample(0:90, n, replace = TRUE),
  pesoano  = round(runif(n, 0.5, 3.0), 4)
)

svy <- Survey$new(
  data   = dt,
  edition = "2023",
  type    = "ech",
  psu    = NULL,
  engine = "data.table",
  weight = add_weight(annual = "pesoano")
)

# Build the processing pipeline step by step
svy <- step_compute(svy,
  # Per capita household income
  income_pc = ht11 / cant_per,
  comment = "Per capita household income (total household income / household size)"
)

svy <- step_recode(svy, region,
  dpto == 1   ~ "Montevideo",
  dpto %in% c(3, 10) ~ "Metropolitan",
  .default = "Interior",
  comment = "Geographic region classification (Montevideo, Metropolitan area, Interior)"
)

svy <- step_compute(svy,
  # Flag for extreme values (top/bottom 0.5%)
  income_flag = ifelse(
    income_pc < quantile(income_pc, 0.005) |
    income_pc > quantile(income_pc, 0.995),
    1, 0
  ),
  comment = "Flag extreme income values for sensitivity analysis"
)

# Convert steps into a recipe
income_recipe <- steps_to_recipe(
  name        = "ECH Per Capita Income with Regional Classification",
  user        = "juan.perez@example.com",
  svy         = svy,
  description = paste(
    "Constructs per capita household income from variable ht11",
    "(total household income) divided by household size (cant_per).",
    "Adds a three-level regional classification and flags extreme",
    "values at the 0.5th and 99.5th percentiles for robustness checks."
  ),
  steps = get_steps(svy),
  topic = "income"
)
```

### Adding metadata before publishing

Good metadata helps other researchers find and trust your recipe:

```{r add-metadata}
# Add categories for discoverability
income_recipe <- income_recipe |>
  add_category("income", "Income distribution and poverty analysis") |>
  add_category("demographics", "Demographic variables and classifications")

# Add user information
author <- recipe_user(
  name  = "Juan Perez",
  email = "juan.perez@example.com",
  type  = "individual"
)
income_recipe <- set_user_info(income_recipe, author)

# Set a version
income_recipe <- set_version(income_recipe, "1.0.0")

# Review the auto-generated documentation
doc <- income_recipe$doc()
doc$pipeline
```

### Publishing to the API

```{r publish}
# Make sure you are logged in
api_login("juan.perez@example.com", "a_secure_password")

# Publish
api_publish_recipe(income_recipe)
# Message: "Recipe published: <assigned-id>"
```

Your recipe is now available to anyone using the metasurvey package. When
another researcher runs `api_list_recipes(svy_type = "ech")`, your income
recipe will appear in the results.

### Saving a local copy

It is always good practice to keep a local copy of your recipes, both
for version control and for offline use:

```{r save-local}
save_recipe(income_recipe, "recipes/ech_income_pc_2023.json")

# Later, reload it:
loaded <- read_recipe("recipes/ech_income_pc_2023.json")
```

## Workflows: sharing complete analyses

Recipes capture data *processing*---the variable construction and
transformation steps. Workflows capture the *estimation* step: the
`svymean`, `svytotal`, and `svyby` calls that produce the final
statistics. Together, they document a complete reproducible analysis.

### Creating a workflow from estimation results

```{r workflow-create}
# Run estimations on the processed survey
results <- workflow(
  list(svy),
  survey::svymean(~income_pc, na.rm = TRUE),
  survey::svyby(~income_pc, ~region, survey::svymean, na.rm = TRUE),
  estimation_type = "annual"
)

# The workflow object is created automatically when you attach it to a survey
# You can also create one manually for publishing:
wf <- RecipeWorkflow$new(
  name        = "Mean Income by Region - ECH 2023",
  user        = "juan.perez@example.com",
  survey_type = "ech",
  edition     = "2023",
  description = paste(
    "Per capita income estimates with regional breakdown.",
    "Uses annual weights and bootstrap replicate standard errors."
  ),
  estimation_type = list("annual"),
  recipe_ids  = list(income_recipe$id),
  calls       = list(
    "svymean(~income_pc, na.rm = TRUE)",
    "svyby(~income_pc, ~region, svymean, na.rm = TRUE)"
  )
)

# Publish the workflow
api_publish_workflow(wf)
```

### Discovering workflows

Workflows are searchable in the same way as recipes:

```{r workflow-search}
# Find all workflows for the ECH
ech_workflows <- api_list_workflows(survey_type = "ech")

# Find workflows that use a specific recipe
related <- api_list_workflows(recipe_id = "ech_income_pc_2023")
```

## The certification system

Not all recipes carry the same level of trust. A recipe published by a
graduate student working on their thesis is valuable, but it has not been
through the same scrutiny as one developed by the INE for official
statistics. The certification system makes this distinction explicit.

### How it works

When a recipe is first published, it receives `community` certification
by default. An institutional member can upgrade it to `reviewed`, and an
institution can grant `official` status:

```{r certification}
# An IECON researcher reviews and certifies a recipe
iecon_member <- recipe_user(
  name = "Maria Garcia",
  type = "institutional_member",
  institution = "Instituto de Economia"
)

# Upgrade to "reviewed"
income_recipe |> certify_recipe(iecon_member, "reviewed")

# If the IECON itself certifies it:
iecon <- recipe_user("Instituto de Economia", type = "institution")
income_recipe |> certify_recipe(iecon, "official")
```

### Trust levels

| Account Type | Trust Level | Can Certify |
|--------------|-------------|-------------|
| Individual | 1 | community only |
| Institutional Member | 2 | reviewed |
| Institution | 3 | official |

This hierarchy maps naturally to how academic research is validated: an
individual proposes, peers review, and institutions endorse.

## A complete scenario

To tie everything together, here is a realistic scenario that shows
how the ecosystem supports collaborative research.

**Maria** works at the IECON and has developed a comprehensive labor
force recipe for the ECH 2023 that follows ILO conventions. She has
tested it against the official INE publications and confirmed that her
estimates match within sampling error:

```{r scenario-maria}
# Maria logs in with her institutional member account
api_login("maria@fcea.edu.uy", "her_password")

# She publishes her recipe
api_publish_recipe(labor_recipe)

# She also publishes the workflow she used for validation
api_publish_workflow(validation_workflow)
```

**Santiago** is a master's student at another university. He is writing
his thesis on youth unemployment trends and needs labor force indicators
for the ECH. Instead of coding them from scratch, he searches the
ecosystem:

```{r scenario-santiago}
# Santiago searches for labor recipes
labor <- api_list_recipes(search = "labor force", svy_type = "ech")

# He finds Maria's recipe, which is certified as "reviewed"
marias_recipe <- labor[[1]]
cat(marias_recipe$name, "\n")
cat("Certification:", marias_recipe$certification$level, "\n")

# He checks the documentation
doc <- marias_recipe$doc()
cat("Input variables needed:\n")
cat(paste(" -", doc$input_variables), sep = "\n")

# He validates it against his data
validation <- marias_recipe$validate(his_ech_2023)

# It passes, so he applies it
his_ech_2023$add_recipe(marias_recipe)
his_ech_2023 <- bake_recipes(his_ech_2023)
```

Santiago saved days of work and can be confident that his variable
definitions match the ILO standards, because Maria already validated
them. His thesis supervisor can verify this by checking the recipe's
certification level and documentation.

**Later**, when the IECON decides to officially endorse a set of standard
recipes for the ECH, they certify Maria's recipe at the `official` level.
From that point on, any researcher who filters by
`certification = "official"` will find it.

## Working offline

The API is not required for day-to-day use of recipes. You can save
recipes as JSON files and share them through any channel---email, Git
repositories, shared drives:

```{r offline}
# Save a recipe locally
save_recipe(my_recipe, "my_recipe.json")

# Share the file with a colleague, who loads it:
loaded <- read_recipe("my_recipe.json")

# Apply it to their data
their_survey$add_recipe(loaded)
their_survey <- bake_recipes(their_survey)
```

The API adds discoverability and a central registry, but the core recipe
functionality works entirely offline.

## Environment variables reference

| Variable | Purpose | Default |
|----------|---------|---------|
| `METASURVEY_API_URL` | API base URL | `https://metasurvey-api-production.up.railway.app` |
| `METASURVEY_TOKEN` | JWT auth token (set automatically on login) | --- |

You can also configure these in R:

```{r config}
# Point to a custom API instance (e.g., for development)
configure_api("https://my-private-api.example.com")

# Or via options
options(metasurvey.api_url = "https://my-private-api.example.com")
```

## Summary

The recipe ecosystem transforms survey data processing from a solitary,
error-prone activity into a collaborative one. By publishing your
recipes, you contribute to a shared knowledge base that benefits the
entire community of survey researchers. By discovering and reusing
existing recipes, you save time and reduce the risk of errors in your
own work.

The key functions to remember:

| Task | Function |
|------|----------|
| Login | `api_login(email, password)` |
| Register | `api_register(name, email, password)` |
| Browse recipes | `api_list_recipes()` |
| Search recipes | `api_list_recipes(search = "...")` |
| Get a recipe | `api_get_recipe(id)` |
| Publish a recipe | `api_publish_recipe(recipe)` |
| Browse workflows | `api_list_workflows()` |
| Publish a workflow | `api_publish_workflow(workflow)` |
| Save locally | `save_recipe(recipe, path)` |
| Load from file | `read_recipe(path)` |
| Logout | `api_logout()` |
