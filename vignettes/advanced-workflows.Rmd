---
title: "Advanced Workflow Techniques"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Advanced Workflow Techniques}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE,
  eval = FALSE
)
```

```{r setup}
library(metasurvey)
library(survey)
library(magrittr)
library(data.table)
library(ggplot2)
library(parallel)
```

## Introduction

This vignette covers advanced workflow techniques in `metasurvey` for complex estimation scenarios. These techniques are essential for production-level survey analysis, particularly in official statistics where robustness, efficiency, and quality are paramount.

Topics covered:
- Complex estimation workflows with multiple surveys
- Variance estimation techniques and adjustments
- Time series and panel estimation workflows  
- Quality assessment and diagnostic workflows
- Performance optimization for large-scale production
- Error handling and robustness techniques

## Multi-Survey Workflows

### Pooled Cross-Sectional Analysis

Combining multiple cross-sectional surveys for increased precision:

```{r pooled-analysis}
# Load multiple years of cross-sectional data
surveys_2020_2023 <- list(
  "2020" = load_survey("survey_2020.csv", svy_edition = "2020", 
                       svy_type = "annual_survey", svy_weight = add_weight(annual = "weight")),
  "2021" = load_survey("survey_2021.csv", svy_edition = "2021",
                       svy_type = "annual_survey", svy_weight = add_weight(annual = "weight")),
  "2022" = load_survey("survey_2022.csv", svy_edition = "2022", 
                       svy_type = "annual_survey", svy_weight = add_weight(annual = "weight")),
  "2023" = load_survey("survey_2023.csv", svy_edition = "2023",
                       svy_type = "annual_survey", svy_weight = add_weight(annual = "weight"))
)

# Apply consistent processing to all surveys
common_recipe <- recipe(
  name = "Common Variables 2020-2023",
  user = "Time Series Team",
  svy = survey_empty(type = "annual_survey", edition = "2020-2023"),
  description = "Harmonized variables for time series analysis",
  
  step_recode(
    "employment_status",
    activity_code == 1 ~ "Employed",
    activity_code == 2 ~ "Unemployed", 
    .default = "Not in Labor Force",
    comment = "Standardized employment status"
  ),
  
  step_compute(
    employed = ifelse(employment_status == "Employed", 1, 0),
    unemployed = ifelse(employment_status == "Unemployed", 1, 0),
    labor_force = employed + unemployed,
    unemployment_rate = unemployed / labor_force,
    comment = "Core labor force indicators"
  )
)

# Process all surveys with common recipe
processed_surveys <- lapply(surveys_2020_2023, function(survey) {
  survey %>%
    add_recipe(common_recipe) %>%
    bake_recipes()
})

# Pooled estimation workflow
pooled_results <- workflow(
  survey = processed_surveys,
  survey::svymean(~unemployment_rate, na.rm = TRUE),
  survey::svytotal(~employed),
  survey::svytotal(~unemployed),
  estimation_type = "annual"
)

# Add time dimension
pooled_results[, year := rep(2020:2023, each = .N/4)]

print(pooled_results)
```

### Integrated Survey Systems

Combining different survey types for comprehensive analysis:

```{r integrated-systems}
# Household and establishment surveys integration
household_survey <- load_survey(
  "household_2023.csv",
  svy_edition = "2023", 
  svy_type = "household_survey",
  svy_weight = add_weight(annual = "hh_weight")
)

establishment_survey <- load_survey(
  "establishment_2023.csv",
  svy_edition = "2023",
  svy_type = "establishment_survey", 
  svy_weight = add_weight(annual = "est_weight")
)

# Process household data
hh_processed <- household_survey %>%
  step_compute(
    employed_formal = employed & has_contract,
    employed_informal = employed & !has_contract,
    comment = "Employment formality status"
  ) %>%
  bake_steps()

# Process establishment data  
est_processed <- establishment_survey %>%
  step_compute(
    formal_jobs = total_employees * formality_rate,
    informal_jobs = total_employees * (1 - formality_rate),
    comment = "Job formality from establishment side"
  ) %>%
  bake_steps()

# Integrated workflow
integrated_results <- list(
  household = workflow(
    survey = list(hh_processed),
    survey::svytotal(~employed_formal),
    survey::svytotal(~employed_informal),
    estimation_type = "annual"
  ),
  establishment = workflow(
    survey = list(est_processed),
    survey::svytotal(~formal_jobs),
    survey::svytotal(~informal_jobs),
    estimation_type = "annual"
  )
)

# Compare household and establishment estimates
comparison <- data.table(
  source = c("Household", "Household", "Establishment", "Establishment"),
  type = c("Formal", "Informal", "Formal", "Informal"),
  estimate = c(
    integrated_results$household[stat == "employed_formal", value],
    integrated_results$household[stat == "employed_informal", value],
    integrated_results$establishment[stat == "formal_jobs", value],
    integrated_results$establishment[stat == "informal_jobs", value]
  )
)

print(comparison)
```

## Advanced Variance Estimation

### Variance Component Models

For complex survey designs with multiple variance sources:

```{r variance-components}
# Multi-stage cluster survey with bootstrap replicates
complex_survey <- load_survey(
  "complex_survey.csv",
  svy_edition = "2023",
  svy_type = "multi_stage",
  svy_weight = add_weight(
    main = add_replicate(
      weight_var = "final_weight",
      replicate_path = "bootstrap_weights.csv",
      replicate_pattern = "rep_[0-9]+",
      replicate_type = "bootstrap"
    )
  ),
  psu = ~psu_id,
  strata = ~strata_id
)

# Workflow with variance decomposition
variance_analysis <- workflow(
  survey = list(complex_survey),
  survey::svymean(~income),
  survey::svyvar(~income),
  survey::svyquantile(~income, quantiles = c(0.1, 0.5, 0.9)),
  estimation_type = "main"
)

# Calculate design effects
simple_survey <- load_survey(
  complex_survey$data,
  svy_edition = "2023",
  svy_type = "simple",
  svy_weight = add_weight(main = "final_weight")
)

simple_results <- workflow(
  survey = list(simple_survey),
  survey::svymean(~income),
  survey::svyvar(~income),
  estimation_type = "main"
)

# Design effect calculation
variance_analysis[, design_effect := se^2 / simple_results[stat == stat, se]^2]
variance_analysis[, effective_n := nrow(complex_survey$data) / design_effect]

print(variance_analysis[, .(stat, value, se, design_effect, effective_n)])
```

### Correlation Adjustments

For overlapping samples or panel components:

```{r correlation-adjustments}
# Panel survey with quarterly-to-monthly estimation
panel_survey <- load_panel_survey(
  path_implantation = "panel_implantation.csv",
  path_follow_up = "panel_followup/",
  svy_type = "rotating_panel",
  svy_weight_implantation = add_weight(quarterly = "q_weight"),
  svy_weight_follow_up = add_weight(monthly = "m_weight")
)

# Extract quarterly surveys
quarterly_surveys <- extract_surveys(panel_survey, quarterly = 1:4)

# Workflow with correlation adjustment
adjusted_results <- workflow(
  survey = quarterly_surveys,
  survey::svymean(~unemployment_rate, na.rm = TRUE),
  survey::svyratio(~unemployed, ~labor_force),
  estimation_type = "quarterly:monthly",
  rho = 0.6,  # estimated correlation between quarters
  R = 5/6     # overlap ratio (5 months overlap in 6-month period)
)

# Compare with unadjusted results
unadjusted_results <- workflow(
  survey = quarterly_surveys,
  survey::svymean(~unemployment_rate, na.rm = TRUE),
  estimation_type = "quarterly"
)

# Variance inflation due to correlation
correlation_impact <- data.table(
  method = c("Adjusted", "Unadjusted"),
  se = c(adjusted_results[1, se], unadjusted_results[1, se]),
  variance_inflation = c(adjusted_results[1, se]^2 / unadjusted_results[1, se]^2, 1.0)
)

print(correlation_impact)
```

### Small Area Estimation

For domain estimates with insufficient sample sizes:

```{r small-area}
# Workflow with small area techniques
small_area_survey <- load_survey(
  "regional_survey.csv",
  svy_edition = "2023",
  svy_type = "regional_survey",
  svy_weight = add_weight(main = "weight")
) %>%
  step_recode(
    "region",
    region_code %in% 1:5 ~ "North",
    region_code %in% 6:10 ~ "Central", 
    region_code %in% 11:15 ~ "South",
    .default = "Other",
    comment = "Regional classification"
  ) %>%
  bake_steps()

# Domain estimation with quality assessment
domain_results <- workflow(
  survey = list(small_area_survey),
  survey::svyby(~income, ~region, survey::svymean),
  survey::svyby(~unemployment_rate, ~region, survey::svymean),
  estimation_type = "main"
)

# Assess domain sample sizes and quality
domain_quality <- domain_results[, .(
  region = region,
  stat = stat,
  estimate = value,
  cv = cv,
  quality = sapply(cv, evaluate_cv),
  # Effective sample size
  eff_n = (value / se)^2
)]

# Flag problematic domains
domain_quality[, publishable := eff_n >= 30 & cv < 0.25]

print(domain_quality)
```

## Time Series Workflows

### Seasonal Adjustment Workflows

```{r seasonal-adjustment}
# Monthly time series with seasonal patterns
monthly_surveys <- lapply(1:24, function(i) {  # 2 years of monthly data
  load_survey(
    paste0("monthly_", i, ".csv"),
    svy_edition = paste0("2022_", sprintf("%02d", ((i-1) %% 12) + 1)),
    svy_type = "monthly_survey",
    svy_weight = add_weight(monthly = "weight")
  ) %>%
  step_compute(
    month = ((i-1) %% 12) + 1,
    year = 2022 + floor((i-1) / 12),
    quarter = ceiling(month / 3),
    comment = "Time identifiers"
  ) %>%
  bake_steps()
})

# Time series estimation workflow
ts_results <- workflow(
  survey = monthly_surveys,
  survey::svymean(~unemployment_rate, na.rm = TRUE),
  survey::svytotal(~employed),
  estimation_type = "monthly"
)

# Add time variables
ts_results[, `:=`(
  month = rep(1:12, 2),
  year = rep(2022:2023, each = 12),
  time_index = 1:.N
)]

# Seasonal analysis
seasonal_pattern <- ts_results[stat == "unemployment_rate", .(
  month = month,
  year = year,
  unemployment_rate = value,
  se = se
)]

# Simple seasonal adjustment (conceptual)
seasonal_pattern[, seasonal_factor := mean(unemployment_rate) / unemployment_rate, by = month]
seasonal_pattern[, adjusted_rate := unemployment_rate * seasonal_factor]

print(head(seasonal_pattern))
```

### Trend Analysis

```{r trend-analysis}
# Trend estimation with multiple time points
trend_workflow <- function(survey_list, reference_periods) {
  results <- workflow(
    survey = survey_list,
    survey::svymean(~unemployment_rate, na.rm = TRUE),
    survey::svymean(~labor_force_participation, na.rm = TRUE),
    estimation_type = "monthly"
  )
  
  # Add time dimension
  results[, period := rep(reference_periods, each = .N / length(reference_periods))]
  
  # Calculate period-to-period changes
  for (var in unique(results$stat)) {
    subset_data <- results[stat == var]
    subset_data[, `:=`(
      change = value - shift(value, 1),
      change_se = sqrt(se^2 + shift(se, 1)^2),  # approximate
      change_significant = abs(change) > 1.96 * change_se
    )]
    results[stat == var, `:=`(
      change = subset_data$change,
      change_se = subset_data$change_se,
      change_significant = subset_data$change_significant
    )]
  }
  
  return(results)
}

# Apply trend analysis
trend_results <- trend_workflow(monthly_surveys, 1:24)

# Significant changes
significant_changes <- trend_results[change_significant == TRUE & !is.na(change)]
print(significant_changes[, .(period, stat, change, change_se)])
```

## Quality Assessment Workflows

### Comprehensive Quality Diagnostics

```{r quality-diagnostics}
# Quality assessment workflow
quality_assessment <- function(survey, estimation_calls) {
  # Run base estimation
  results <- do.call(workflow, c(list(survey = list(survey)), estimation_calls, 
                                list(estimation_type = "main")))
  
  # Add quality metrics
  results[, `:=`(
    quality_cv = sapply(cv, evaluate_cv),
    effective_n = (value / se)^2,
    design_effect = se^2 / (value / sqrt(nrow(survey$data)))^2,  # approximate
    publishable = cv < 0.25 & (value / se)^2 >= 30
  )]
  
  # Flag potential issues
  results[, `:=`(
    high_cv = cv > 0.15,
    low_sample = effective_n < 50,
    high_deff = design_effect > 3,
    extreme_value = abs(value - median(value, na.rm = TRUE)) > 3 * mad(value, na.rm = TRUE)
  )]
  
  # Overall quality score
  results[, quality_score := 
    5 * (cv < 0.05) +
    4 * (cv >= 0.05 & cv < 0.10) +
    3 * (cv >= 0.10 & cv < 0.15) +
    2 * (cv >= 0.15 & cv < 0.25) +
    1 * (cv >= 0.25 & cv < 0.35) +
    0 * (cv >= 0.35)
  ]
  
  return(results)
}

# Apply quality assessment
quality_results <- quality_assessment(
  complex_survey,
  list(
    survey::svymean(~income),
    survey::svytotal(~employed),
    survey::svyratio(~unemployed, ~labor_force)
  )
)

# Quality summary
quality_summary <- quality_results[, .(
  total_estimates = .N,
  publishable = sum(publishable),
  high_quality = sum(quality_score >= 4),
  needs_attention = sum(high_cv | low_sample | high_deff),
  avg_cv = mean(cv),
  avg_quality_score = mean(quality_score)
)]

print(quality_summary)
```

### Outlier Detection

```{r outlier-detection}
# Outlier detection in survey estimates
outlier_detection <- function(results_series) {
  # Time series outlier detection
  for (var in unique(results_series$stat)) {
    subset_data <- results_series[stat == var]
    
    # Calculate rolling statistics
    subset_data[, `:=`(
      rolling_mean = frollmean(value, n = 3, align = "center"),
      rolling_sd = frollapply(value, n = 3, FUN = sd, align = "center")
    )]
    
    # Outlier flags
    subset_data[, `:=`(
      statistical_outlier = abs(value - rolling_mean) > 2 * rolling_sd,
      trend_break = abs(value - shift(value, 1)) > 3 * se,
      level_shift = abs(value - rolling_mean) > 3 * se
    )]
    
    # Update main results
    results_series[stat == var, `:=`(
      statistical_outlier = subset_data$statistical_outlier,
      trend_break = subset_data$trend_break,
      level_shift = subset_data$level_shift
    )]
  }
  
  return(results_series)
}

# Apply outlier detection to time series
ts_with_outliers <- outlier_detection(ts_results)

# Summary of detected outliers
outlier_summary <- ts_with_outliers[, .(
  period = time_index,
  stat = stat,
  value = value,
  statistical_outlier = statistical_outlier,
  trend_break = trend_break,
  level_shift = level_shift
)][statistical_outlier == TRUE | trend_break == TRUE | level_shift == TRUE]

print(outlier_summary)
```

## Performance Optimization

### Parallel Processing

```{r parallel-processing}
# Set up parallel processing
n_cores <- parallel::detectCores() - 1
cl <- parallel::makeCluster(n_cores)

# Parallel survey processing
parallel_survey_processing <- function(survey_paths, recipe, n_cores = 4) {
  # Set up cluster
  cl <- parallel::makeCluster(n_cores)
  on.exit(parallel::stopCluster(cl))
  
  # Load required packages on workers
  parallel::clusterEvalQ(cl, {
    library(metasurvey)
    library(survey)
    library(magrittr)
  })
  
  # Export objects to workers
  parallel::clusterExport(cl, c("recipe"), envir = environment())
  
  # Process surveys in parallel
  results <- parallel::parLapply(cl, survey_paths, function(path) {
    survey <- load_survey(
      path,
      svy_edition = basename(path),
      svy_type = "parallel_survey",
      svy_weight = add_weight(main = "weight")
    )
    
    processed <- survey %>%
      add_recipe(recipe) %>%
      bake_recipes()
    
    # Run workflow
    result <- workflow(
      survey = list(processed),
      survey::svymean(~unemployment_rate, na.rm = TRUE),
      survey::svytotal(~employed),
      estimation_type = "main"
    )
    
    return(result)
  })
  
  # Combine results
  combined_results <- rbindlist(results)
  return(combined_results)
}

# Example usage (not run)
# survey_files <- list.files("surveys/", pattern = "*.csv", full.names = TRUE)
# parallel_results <- parallel_survey_processing(survey_files, common_recipe, n_cores = 4)
```

### Memory-Efficient Processing

```{r memory-efficient}
# Memory-efficient processing for large surveys
large_survey_workflow <- function(data_path, chunk_size = 10000) {
  # Process data in chunks
  chunk_results <- list()
  
  # Read data info without loading full dataset
  data_info <- data.table::fread(data_path, nrows = 0)
  total_rows <- data.table::fread(data_path, select = 1L, showProgress = FALSE)[, .N]
  n_chunks <- ceiling(total_rows / chunk_size)
  
  for (i in 1:n_chunks) {
    skip_rows <- (i - 1) * chunk_size
    
    # Read chunk
    chunk_data <- data.table::fread(
      data_path,
      skip = skip_rows,
      nrows = chunk_size,
      showProgress = FALSE
    )
    
    if (nrow(chunk_data) == 0) break
    
    # Process chunk
    chunk_survey <- load_survey(
      data = chunk_data,
      svy_edition = "2023",
      svy_type = "large_survey",
      svy_weight = add_weight(main = "weight")
    ) %>%
      step_compute(
        employed = ifelse(employment_status == 1, 1, 0),
        comment = "Employment indicator"
      ) %>%
      bake_steps()
    
    # Estimate for chunk
    chunk_result <- workflow(
      survey = list(chunk_survey),
      survey::svytotal(~employed),
      estimation_type = "main"
    )
    
    chunk_results[[i]] <- chunk_result
    
    # Clean up
    rm(chunk_data, chunk_survey)
    gc()
  }
  
  # Combine chunk results (weighted average)
  combined_results <- rbindlist(chunk_results)
  
  return(combined_results)
}
```

### Caching and Memoization

```{r caching}
# Caching system for expensive operations
create_workflow_cache <- function() {
  cache <- new.env(parent = emptyenv())
  
  cached_workflow <- function(survey, ..., cache_key = NULL) {
    # Generate cache key if not provided
    if (is.null(cache_key)) {
      cache_key <- digest::digest(list(
        survey_hash = digest::digest(survey$data),
        calls = list(...),
        survey_metadata = list(survey$type, survey$edition)
      ))
    }
    
    # Check cache
    if (exists(cache_key, envir = cache)) {
      message("Using cached result")
      return(get(cache_key, envir = cache))
    }
    
    # Compute result
    result <- workflow(survey = list(survey), ..., estimation_type = "main")
    
    # Store in cache
    assign(cache_key, result, envir = cache)
    
    return(result)
  }
  
  # Cache management functions
  list(
    workflow = cached_workflow,
    clear = function() rm(list = ls(envir = cache), envir = cache),
    size = function() length(ls(envir = cache)),
    keys = function() ls(envir = cache)
  )
}

# Create and use cache
workflow_cache <- create_workflow_cache()

# First call - computed
# result1 <- workflow_cache$workflow(
#   survey_data,
#   survey::svymean(~income),
#   cache_key = "income_analysis_2023"
# )

# Second call - cached
# result2 <- workflow_cache$workflow(
#   survey_data,
#   survey::svymean(~income), 
#   cache_key = "income_analysis_2023"
# )
```

## Error Handling and Robustness

### Comprehensive Error Handling

```{r error-handling}
# Robust workflow with error handling
robust_workflow <- function(survey, estimation_calls, max_retries = 3) {
  results <- list()
  errors <- list()
  
  for (i in seq_along(estimation_calls)) {
    call_name <- paste0("call_", i)
    retry_count <- 0
    
    while (retry_count < max_retries) {
      tryCatch({
        # Attempt estimation
        result <- workflow(
          survey = list(survey),
          estimation_calls[[i]],
          estimation_type = "main"
        )
        
        # Validate result
        if (any(is.na(result$value)) || any(result$se <= 0)) {
          stop("Invalid estimation result")
        }
        
        results[[call_name]] <- result
        break
        
      }, error = function(e) {
        retry_count <<- retry_count + 1
        
        if (retry_count >= max_retries) {
          errors[[call_name]] <- list(
            error = e$message,
            call = deparse(estimation_calls[[i]]),
            retries = retry_count
          )
        } else {
          # Wait before retry
          Sys.sleep(0.1 * retry_count)
        }
      })
    }
  }
  
  return(list(results = results, errors = errors))
}

# Example usage
estimation_calls <- list(
  survey::svymean(~income),
  survey::svytotal(~employed),
  survey::svyratio(~unemployed, ~labor_force)
)

# robust_results <- robust_workflow(survey_data, estimation_calls)
```

### Validation and Consistency Checks

```{r validation}
# Validation framework for survey results
validate_survey_results <- function(results) {
  validation_report <- list()
  
  # Range checks
  validation_report$range_checks <- results[, .(
    stat = stat,
    within_bounds = case_when(
      grepl("rate|ratio", stat) ~ value >= 0 & value <= 1,
      grepl("total|count", stat) ~ value >= 0,
      TRUE ~ TRUE
    ),
    reasonable_cv = cv > 0 & cv < 1
  )]
  
  # Consistency checks
  if (any(grepl("unemployment_rate", results$stat)) && 
      any(grepl("employment_rate", results$stat))) {
    unemp_rate <- results[stat == "unemployment_rate", value]
    emp_rate <- results[stat == "employment_rate", value]
    
    validation_report$consistency_checks <- data.table(
      check = "Employment + Unemployment rates",
      consistent = (unemp_rate + emp_rate) <= 1.01,  # Allow small tolerance
      actual_sum = unemp_rate + emp_rate
    )
  }
  
  # Statistical significance tests
  validation_report$significance_tests <- results[, .(
    stat = stat,
    significantly_different_from_zero = abs(value) > 1.96 * se,
    t_statistic = value / se
  )]
  
  # Overall validation status
  validation_report$overall_status <- list(
    all_range_checks_passed = all(validation_report$range_checks$within_bounds, na.rm = TRUE),
    all_cv_reasonable = all(validation_report$range_checks$reasonable_cv, na.rm = TRUE),
    any_consistency_issues = any(!validation_report$consistency_checks$consistent, na.rm = TRUE)
  )
  
  return(validation_report)
}

# Apply validation
# validation_results <- validate_survey_results(survey_results)
```

## Production Workflows

### Automated Quality Reports

```{r quality-reports}
# Generate comprehensive quality report
generate_quality_report <- function(results, survey_metadata) {
  report <- list()
  
  # Executive summary
  report$executive_summary <- list(
    total_estimates = nrow(results),
    publishable_estimates = sum(results$cv < 0.25, na.rm = TRUE),
    high_quality_estimates = sum(results$cv < 0.10, na.rm = TRUE),
    avg_cv = mean(results$cv, na.rm = TRUE),
    max_cv = max(results$cv, na.rm = TRUE)
  )
  
  # Detailed quality metrics
  report$quality_metrics <- results[, .(
    stat = stat,
    estimate = value,
    se = se,
    cv = cv,
    quality_level = sapply(cv, evaluate_cv),
    effective_n = (value / se)^2,
    publishable = cv < 0.25
  )]
  
  # Quality distribution
  report$quality_distribution <- report$quality_metrics[, .N, by = quality_level]
  
  # Recommendations
  problematic_estimates <- report$quality_metrics[cv >= 0.25]
  
  if (nrow(problematic_estimates) > 0) {
    report$recommendations <- list(
      action_required = "Some estimates have high coefficients of variation",
      affected_estimates = problematic_estimates$stat,
      suggestions = c(
        "Consider combining categories or time periods",
        "Use model-based estimation for small domains",
        "Increase sample size in future surveys",
        "Apply statistical disclosure control"
      )
    )
  } else {
    report$recommendations <- list(
      action_required = "No action required",
      status = "All estimates meet quality standards"
    )
  }
  
  return(report)
}

# Generate report
# quality_report <- generate_quality_report(survey_results, survey_metadata)
```

### Publication Workflow

```{r publication-workflow}
# End-to-end publication workflow
publication_workflow <- function(survey_data, recipes, output_path) {
  # Initialize results storage
  publication_results <- list()
  
  # Step 1: Data processing
  message("Step 1: Processing survey data...")
  processed_survey <- survey_data
  
  for (recipe in recipes) {
    processed_survey <- processed_survey %>%
      add_recipe(recipe)
  }
  
  processed_survey <- bake_recipes(processed_survey)
  
  # Step 2: Core estimations
  message("Step 2: Running core estimations...")
  core_results <- workflow(
    survey = list(processed_survey),
    survey::svymean(~unemployment_rate, na.rm = TRUE),
    survey::svytotal(~employed),
    survey::svytotal(~unemployed),
    survey::svymean(~labor_force_participation, na.rm = TRUE),
    estimation_type = "main"
  )
  
  # Step 3: Quality assessment
  message("Step 3: Quality assessment...")
  quality_assessment <- validate_survey_results(core_results)
  
  # Step 4: Domain estimates
  message("Step 4: Domain estimates...")
  domain_results <- workflow(
    survey = list(processed_survey),
    survey::svyby(~unemployment_rate, ~region, survey::svymean),
    survey::svyby(~unemployment_rate, ~age_group, survey::svymean),
    estimation_type = "main"
  )
  
  # Step 5: Generate outputs
  message("Step 5: Generating outputs...")
  
  # Main estimates table
  main_table <- core_results[, .(
    Indicator = stat,
    Estimate = round(value, 4),
    `Standard Error` = round(se, 4),
    `Coefficient of Variation` = round(cv, 4),
    `95% CI Lower` = round(confint_lower, 4),
    `95% CI Upper` = round(confint_upper, 4),
    Quality = sapply(cv, evaluate_cv)
  )]
  
  # Save outputs
  data.table::fwrite(main_table, file.path(output_path, "main_estimates.csv"))
  data.table::fwrite(domain_results, file.path(output_path, "domain_estimates.csv"))
  
  # Save quality report
  quality_report <- generate_quality_report(core_results, list())
  jsonlite::write_json(quality_report, file.path(output_path, "quality_report.json"), 
                      pretty = TRUE, auto_unbox = TRUE)
  
  message("Publication workflow completed successfully!")
  
  return(list(
    main_estimates = main_table,
    domain_estimates = domain_results,
    quality_report = quality_report
  ))
}

# Example usage
# publication_results <- publication_workflow(
#   survey_data = processed_survey,
#   recipes = list(demographic_recipe, labor_force_recipe),
#   output_path = "publication_outputs/"
# )
```

## Best Practices for Advanced Workflows

### 1. Modular Design
```{r bp-modular}
# Break complex workflows into reusable components
# Use functions for repeated operations
# Implement consistent interfaces across workflow components
```

### 2. Error Prevention
```{r bp-error-prevention}
# Validate inputs before processing
# Use type checking and range validation
# Implement graceful degradation for edge cases
```

### 3. Performance Monitoring
```{r bp-performance}
# Monitor memory usage and execution time
# Profile bottlenecks in complex workflows
# Implement progress reporting for long-running operations
```

### 4. Documentation and Logging
```{r bp-documentation}
# Log all processing steps and decisions
# Document assumptions and limitations
# Maintain audit trail for reproducibility
```

### 5. Testing and Validation
```{r bp-testing}
# Test workflows with known datasets
# Validate against external benchmarks
# Implement automated quality checks
```

## Conclusion

Advanced workflow techniques in `metasurvey` enable:

- **Robust production systems** that handle real-world complexity
- **Quality assurance** through automated validation and diagnostics
- **Performance optimization** for large-scale survey processing
- **Error resilience** through comprehensive error handling
- **Reproducible research** through systematic workflow design

These techniques are essential for operational survey analysis environments where reliability, accuracy, and efficiency are critical requirements.

The combination of these advanced techniques with the foundational concepts from previous vignettes provides a complete toolkit for professional survey analysis using `metasurvey`.
