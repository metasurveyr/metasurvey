---
title: "ECH Case Study: From STATA to R"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ECH Case Study: From STATA to R}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE
)
```

## The ECH Compatibility Problem

Uruguay's *Encuesta Continua de Hogares* (ECH) is published annually by the
Instituto Nacional de Estadistica (INE). Over the years, INE has changed
variable names, codebook definitions, and module structure across editions.
Researchers who work with ECH data must *compatibilize* variables---mapping
different naming conventions to a common schema---before any cross-year
analysis is possible.

In Uruguayan academia, this work has been done in STATA for over 30 years.
A typical compatibility pipeline consists of approximately
**8 .do files per year**, covering:

1. `2_correc_datos.do` -- Load raw data, merge person and household files,
   fix variable names
2. `3_compatibilizacion_mod_1_4.do` -- Harmonize demographics, health,
   education, and labor modules
3. `4_ingreso_ht11.do` -- Construct household income (`ht11`)
4. `5_descomp_fuentes.do` -- Decompose income by source
5. `6_ingreso_ht11_sss.do` -- Social security adjustments
6. `7_check_ingr.do` -- Validate income variables
7. `8_arregla_base_comp.do` -- Final dataset preparation
8. `9_labels.do` -- Apply value labels

Multiplied across 30+ years, that is over **240 STATA scripts** doing
essentially the same task: mapping raw ECH variables to a common schema.

**metasurvey solves this with recipes.** Instead of maintaining hundreds of
`.do` files, you write a single recipe that encodes the transformation logic
and can be applied to any ECH edition.

## STATA vs metasurvey: Side by Side

Here is a fragment of a typical STATA compatibility script for sex, age,
and relationship:

```stata
* STATA: Typical ECH compatibility script

* sexo
g bc_pe2 = e26

* edad
g bc_pe3 = e27

* parentesco
g bc_pe4 = -9
  replace bc_pe4 = 1 if e31 == 1
  replace bc_pe4 = 2 if e31 == 2
  replace bc_pe4 = 3 if e31 == 3 | e31 == 4 | e31 == 5
  replace bc_pe4 = 4 if e31 == 7 | e31 == 8
  replace bc_pe4 = 5 if e31 == 6 | e31 == 9 | e31 == 10 | e31 == 11 | e31 == 12
  replace bc_pe4 = 6 if e31 == 13
  replace bc_pe4 = 7 if e31 == 14
```

The equivalent in metasurvey:

```r
svy <- step_rename(svy, sex = e26, age = e27)

svy <- step_recode(svy, relationship,
  e31 == 1                ~ "Head",
  e31 == 2                ~ "Spouse",
  e31 %in% 3:5            ~ "Child",
  e31 %in% c(7, 8)        ~ "Parent",
  e31 %in% c(6, 9:12)     ~ "Other relative",
  e31 == 13               ~ "Domestic service",
  e31 == 14               ~ "Non-relative",
  .default = NA_character_,
  comment = "Relationship to head of household"
)
```

The key differences:

| Aspect | STATA `.do` file | metasurvey recipe |
|--------|-----------------|------------------|
| Format | Flat script with hardcoded paths | Portable JSON with metadata |
| Validation | Manual `assert` checks | Automatic `validate()` method |
| Documentation | Comments in code | Auto-generated `doc()` method |
| Sharing | Copy files via email/server | Registry with search and versioning |
| Reproducibility | Depends on file paths and environment | Self-contained, any machine |
| Cross-edition | Duplicate script per year | One recipe, multiple editions |

## Simulating ECH-Like Data

For this vignette we simulate data that mirrors the structure of a real ECH
microdata file. The simulated variables follow the same naming conventions
and value ranges used by INE.

```{r simulate}
library(metasurvey)
library(data.table)

set.seed(2023)
n <- 500

ech_sim <- data.table(
  numero   = rep(1:100, each = 5),
  nper     = rep(1:5, 100),
  e26      = sample(c(1, 2), n, replace = TRUE),
  e27      = sample(0:90, n, replace = TRUE),
  e31      = sample(1:14, n, replace = TRUE,
                    prob = c(0.20, 0.15, 0.25, 0.05, 0.03,
                             0.02, 0.03, 0.02, 0.02, 0.01,
                             0.01, 0.01, 0.05, 0.15)),
  pobpcoac = sample(c(2, 3, 4, 5, 6, 7), n, replace = TRUE,
                    prob = c(0.50, 0.03, 0.02, 0.02, 0.30, 0.13)),
  e51      = sample(1:14, n, replace = TRUE),
  ht11     = round(runif(n, 5000, 120000)),
  dpto     = sample(1:19, n, replace = TRUE),
  pesoano  = round(runif(n, 0.5, 3.0), 4)
)

svy <- Survey$new(
  data    = ech_sim,
  edition = "2023",
  type    = "ech",
  psu     = NULL,
  engine  = "data.table",
  weight  = add_weight(annual = "pesoano")
)

head(get_data(svy), 3)
```

## Step 1: Demographic Variables

Rename raw INE codes to readable names and recode categorical variables:

```{r demographics}
# Recode sex from INE codes (e26: 1=Male, 2=Female)
svy <- step_recode(svy, sex,
  e26 == 1 ~ "Male",
  e26 == 2 ~ "Female",
  .default = NA_character_,
  comment = "Sex: 1=Male, 2=Female (INE e26)"
)

# Recode age groups (standard ECH grouping, e27 = age)
svy <- step_recode(svy, age_group,
  e27 < 14 ~ "Child (0-13)",
  e27 < 25 ~ "Youth (14-24)",
  e27 < 45 ~ "Adult (25-44)",
  e27 < 65 ~ "Mature (45-64)",
  .default = "Senior (65+)",
  .to_factor = TRUE,
  ordered = TRUE,
  comment = "Standard age groups for labor statistics"
)
```

## Step 2: Labor Force Classification

The variable `POBPCOAC` (Poblacion por condicion de actividad) is the
central labor status classification in the ECH. INE codes:

- 2 = Employed
- 3-5 = Unemployed (various subcategories)
- 6-7 = Inactive

This mirrors the ILO standard labor force framework:

```{r labor}
svy <- step_recode(svy, labor_status,
  pobpcoac == 2      ~ "Employed",
  pobpcoac %in% 3:5  ~ "Unemployed",
  pobpcoac %in% 6:7  ~ "Inactive",
  .default = NA_character_,
  comment = "ILO labor force status from POBPCOAC"
)

# Create binary indicators
svy <- step_compute(svy,
  employed     = ifelse(pobpcoac == 2, 1L, 0L),
  unemployed   = ifelse(pobpcoac %in% 3:5, 1L, 0L),
  active       = ifelse(pobpcoac %in% 2:5, 1L, 0L),
  working_age  = ifelse(e27 >= 14, 1L, 0L),
  comment = "Labor force binary indicators"
)
```

## Step 3: Income Variables

Construct income indicators following the standard methodology used with
ECH data:

```{r income}
svy <- step_compute(svy,
  income_pc    = ht11 / 5,
  income_thousands = ht11 / 1000,
  log_income   = log(ht11 + 1),
  comment = "Income transformations"
)
```

## Step 4: Geographic Classification

```{r geography}
dept_names <- data.table(
  dpto = 1:19,
  department = c("Montevideo", "Artigas", "Canelones", "Cerro Largo",
                 "Colonia", "Durazno", "Flores", "Florida", "Lavalleja",
                 "Maldonado", "Paysandu", "Rio Negro", "Rivera", "Rocha",
                 "Salto", "San Jose", "Soriano", "Tacuarembo",
                 "Treinta y Tres"),
  region = c("Montevideo", rep("Interior", 18))
)

svy <- step_join(svy,
  dept_names,
  by = "dpto",
  type = "left",
  comment = "Department names and region classification"
)
```

## Building the Recipe

Convert all transformations into a portable recipe:

```{r recipe}
ech_recipe <- steps_to_recipe(
  name        = "ECH Labor Market Indicators",
  user        = "Research Team",
  svy         = svy,
  description = paste(
    "Standard labor market indicators for the ECH.",
    "Includes demographic recoding, ILO labor classification,",
    "income transformations, and geographic joins."
  ),
  steps       = get_steps(svy),
  topic       = "labor"
)

ech_recipe
```

### Auto-Documentation

```{r recipe-doc}
doc <- ech_recipe$doc()

# What variables does the recipe need?
doc$input_variables

# What variables does it create?
doc$output_variables
```

### Publishing to the Registry

Publish the recipe so others can discover and reuse it:

```{r recipe-publish}
# Set up a local registry
set_backend("local", path = tempfile(fileext = ".json"))
get_backend()$publish(ech_recipe)

# Now anyone can find it
found <- search_recipes("labor")
cat("Found", length(found), "recipe(s)\n")
```

## Estimation with workflow()

Now compute standard labor market indicators:

```{r estimation}
# Mean household income
result_income <- workflow(
  list(svy),
  survey::svymean(~ht11, na.rm = TRUE),
  estimation_type = "annual"
)

result_income
```

```{r estimation-labor}
# Employment rate (proportion employed among total population)
result_employment <- workflow(
  list(svy),
  survey::svymean(~employed, na.rm = TRUE),
  estimation_type = "annual"
)

result_employment
```

### Domain Estimation

Compute estimates by subpopulation:

```{r domain}
# Mean income by region
income_region <- workflow(
  list(svy),
  survey::svyby(~ht11, ~region, survey::svymean, na.rm = TRUE),
  estimation_type = "annual"
)

income_region
```

```{r domain-sex}
# Employment by sex
employment_sex <- workflow(
  list(svy),
  survey::svyby(~employed, ~sex, survey::svymean, na.rm = TRUE),
  estimation_type = "annual"
)

employment_sex
```

### Quality Assessment

```{r quality}
results_all <- workflow(
  list(svy),
  survey::svymean(~ht11, na.rm = TRUE),
  survey::svymean(~employed, na.rm = TRUE),
  estimation_type = "annual"
)

for (i in seq_len(nrow(results_all))) {
  cv_pct <- results_all$cv[i] * 100
  cat(
    results_all$stat[i], ":",
    round(cv_pct, 1), "% CV -",
    evaluate_cv(cv_pct), "\n"
  )
}
```

## Reproducibility: Same Recipe, Different Edition

The power of recipes is applying them unchanged to new data. Here we
simulate a "2024 edition" with the same structure:

```{r reproduce}
set.seed(2024)
ech_2024 <- data.table(
  numero   = rep(1:100, each = 5),
  nper     = rep(1:5, 100),
  e26      = sample(c(1, 2), n, replace = TRUE),
  e27      = sample(0:90, n, replace = TRUE),
  e31      = sample(1:14, n, replace = TRUE,
                    prob = c(0.20, 0.15, 0.25, 0.05, 0.03,
                             0.02, 0.03, 0.02, 0.02, 0.01,
                             0.01, 0.01, 0.05, 0.15)),
  pobpcoac = sample(c(2, 3, 4, 5, 6, 7), n, replace = TRUE,
                    prob = c(0.50, 0.03, 0.02, 0.02, 0.30, 0.13)),
  e51      = sample(1:14, n, replace = TRUE),
  ht11     = round(runif(n, 5500, 130000)),
  dpto     = sample(1:19, n, replace = TRUE),
  pesoano  = round(runif(n, 0.5, 3.0), 4)
)

svy_2024 <- Survey$new(
  data = ech_2024, edition = "2024", type = "ech",
  psu = NULL, engine = "data.table",
  weight = add_weight(annual = "pesoano")
)

# Apply the same recipe
svy_2024 <- add_recipe(svy_2024, ech_recipe)
svy_2024 <- bake_recipes(svy_2024)

# Estimate
result_2024 <- workflow(
  list(svy_2024),
  survey::svymean(~ht11, na.rm = TRUE),
  survey::svymean(~employed, na.rm = TRUE),
  estimation_type = "annual"
)

result_2024
```

Same recipe, different data, consistent methodology.

## For STATA Users: Quick Reference

If you are transitioning from STATA to R for survey analysis, here is a
mapping of common operations:

| STATA | metasurvey | Notes |
|-------|-----------|-------|
| `gen var = expr` | `step_compute(svy, var = expr)` | Lazy by default; call `bake_steps()` to execute |
| `replace var = x if cond` | `step_compute(svy, var = ifelse(cond, x, var))` | Conditional assignment |
| `recode var (old=new)` | `step_recode(svy, new_var, old == val ~ "label")` | Creates a new variable |
| `rename old new` | `step_rename(svy, new = old)` | |
| `drop var1 var2` | `step_remove(svy, var1, var2)` | |
| `merge using file` | `step_join(svy, data, by = "key")` | Left join by default |
| `svy: mean var` | `workflow(list(svy), svymean(~var))` | Returns data.table with SE, CV |
| `svy: total var` | `workflow(list(svy), svytotal(~var))` | |
| `svy: mean var, over(group)` | `workflow(list(svy), svyby(~var, ~group, svymean))` | |
| `.do` file | `steps_to_recipe()` + publish | Portable, discoverable, version-controlled |
| `use "data.dta"` | `load_survey(path = "data.dta")` | Reads STATA, CSV, RDS, etc. |

### Key Differences

1. **Lazy evaluation**: In STATA, commands execute immediately. In
   metasurvey, steps are recorded and executed together with `bake_steps()`.
   This enables validation and optimization before execution.

2. **Immutability**: metasurvey creates new variables instead of modifying
   existing ones. `step_recode()` creates a new column; it does not
   overwrite the source variable.

3. **Design awareness**: Survey weights and design are attached to the
   `Survey` object. You never need to prefix commands with `svy:` or
   remember to set the design---`workflow()` handles it automatically.

4. **Recipes vs .do files**: Recipes are self-documenting (via `doc()`),
   self-validating (via `validate()`), and discoverable (via the registry).
   A `.do` file is just a script; a recipe is a structured, portable object.

## Next Steps

- **[Creating and Sharing Recipes](recipes.html)** -- Learn about recipe registries, certification, and discovery
- **[Estimation Workflows](workflows-and-estimation.html)** -- Deep dive into `workflow()` and `RecipeWorkflow`
- **[Rotating Panels and PoolSurvey](panel-analysis.html)** -- Work with the ECH's rotating panel structure
- **[Getting Started](getting-started.html)** -- Review the basics
