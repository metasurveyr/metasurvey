---
title: "Testing and Validating Survey Workflows"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Testing and Validating Survey Workflows}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE,
  eval = FALSE
)
```

```{r setup}
library(metasurvey)
library(survey)
library(data.table)
```

## Introduction

Survey data processing pipelines must be validated to ensure estimates are
correct and reproducible. This vignette covers strategies for testing and
validating metasurvey workflows.

## Inspecting Survey Objects

### Checking metadata

```{r check-metadata}
# Create a survey object
data("api", package = "survey")
dt <- data.table(apistrat)

svy <- Survey$new(
  data = dt,
  edition = "2023",
  type = "test",
  psu = NULL,
  engine = "data.table",
  weight = add_weight(annual = "pw")
)

# Print metadata summary
print(svy)

# Access individual components
get_data(svy)
get_edition(svy)
get_type(svy)
get_steps(svy)
```

### Verifying survey design

```{r check-design}
# Display design details
cat_design(svy)

# Check design type (weighted, replicate, etc.)
cat_design_type(svy, "annual")
```

## Validating Steps

### Step-by-step verification

When building complex pipelines, verify each step independently:

```{r validate-steps}
# Step 1: Compute new variable
svy1 <- step_compute(svy,
  api_diff = api00 - api99,
  comment = "API score difference"
)

# Verify step was recorded
steps <- get_steps(svy1)
cat("Number of steps:", length(steps), "\n")

# Step 2: Recode
svy2 <- step_recode(svy1, improvement,
  api_diff > 0 ~ "Improved",
  api_diff == 0 ~ "No change",
  api_diff < 0 ~ "Declined",
  .default = "Missing",
  comment = "Categorize school improvement"
)

# Check the new column exists
stopifnot("improvement" %in% names(svy2$data))

# Bake compute steps
svy_baked <- bake_steps(svy1)

# Verify computed values
stopifnot(all.equal(
  svy_baked$data$api_diff,
  svy_baked$data$api00 - svy_baked$data$api99
))
```

### Visualizing the pipeline

```{r visualize}
# View the step graph (requires visNetwork)
view_graph(svy2)
```

## Comparing Estimation Approaches

### Cross-validating with the survey package

Compare metasurvey workflow results with direct survey package calls:

```{r cross-validate}
# Method 1: Direct survey package
design <- svydesign(id = ~1, weights = ~pw, data = dt)
direct_mean <- svymean(~api00, design)

# Method 2: metasurvey workflow
wf_result <- workflow(
  survey = list(svy),
  svymean(~api00, na.rm = TRUE),
  estimation_type = "annual"
)

# Compare results
cat("Direct estimate:", coef(direct_mean), "\n")
cat("Workflow estimate:", wf_result$value, "\n")
```

## Testing Recipes

### Recipe consistency check

```{r test-recipes}
# Create a recipe
my_recipe <- recipe(
  name = "Test Recipe",
  user = "QA Team",
  svy = svy,
  description = "Recipe for validation testing",
  step_compute(svy, score_ratio = api00 / api99)
)

# Verify recipe structure
stopifnot(inherits(my_recipe, "Recipe"))
stopifnot(my_recipe$name == "Test Recipe")
stopifnot(length(my_recipe$steps) > 0)
```

### JSON round-trip validation

```{r json-roundtrip}
# Save to temporary file
tmp <- tempfile(fileext = ".json")
save_recipe(my_recipe, tmp)

# Verify file was created
stopifnot(file.exists(tmp))

# Read back
loaded_content <- jsonlite::read_json(tmp, simplifyVector = TRUE)
stopifnot(loaded_content$name == "Test Recipe")

# Cleanup
unlink(tmp)
```

## Quality Assessment with CV

The coefficient of variation (CV) helps assess estimation quality:

```{r cv-assessment}
# After running a workflow, evaluate CVs
results <- workflow(
  survey = list(svy),
  svymean(~api00, na.rm = TRUE),
  estimation_type = "annual"
)

# Check CV thresholds
# cv < 15%: Reliable
# 15% <= cv < 30%: Use with caution
# cv >= 30%: Unreliable
evaluate_cv(results$cv)
```

## Checklist for Production Workflows

Before deploying a survey processing pipeline:

1. **Data integrity**: Verify row counts, column names, and data types after
   each step
2. **Weight validation**: Ensure weight columns exist and are positive
3. **Design verification**: Confirm the survey design matches the expected
   specification (PSU, strata, weights)
4. **Recipe reproducibility**: Save and reload recipes to verify JSON
   serialization works
5. **Cross-validation**: Compare key estimates with known published values
   or alternative calculation methods
6. **CV thresholds**: Flag estimates with high coefficients of variation

```{r production-checklist}
# Example validation function for a pipeline
validate_pipeline <- function(svy) {
  checks <- list()

  # Check data exists
  checks$has_data <- !is.null(get_data(svy))

  # Check data has rows
  checks$has_rows <- nrow(get_data(svy)) > 0

  # Check weight columns exist
  weight_names <- unlist(svy$weight)
  checks$weights_present <- all(
    weight_names[is.character(weight_names)] %in% names(get_data(svy))
  )

  # Check design is set
  checks$has_design <- length(svy$design) > 0

  # Report
  passed <- all(unlist(checks))
  if (passed) {
    message("All validation checks passed")
  } else {
    failed <- names(checks)[!unlist(checks)]
    warning("Failed checks: ", paste(failed, collapse = ", "))
  }

  invisible(checks)
}

# Usage
validate_pipeline(svy)
```
