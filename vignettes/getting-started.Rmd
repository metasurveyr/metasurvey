---
title: "Getting Started with metasurvey"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting Started with metasurvey}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
link-citations: true
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE
)
```

## Introduction

Working with household survey microdata involves a lot of repetitive
processing: recoding categorical variables, constructing indicators,
joining external data, and computing survey-weighted estimates. Each
researcher writes their own version of these transformations, and the
code is rarely shared or documented in a way that others can reuse.

**metasurvey** addresses this by providing a layer of metaprogramming on
top of the `survey` package [@lumley2004]. Instead of writing one-off
scripts, you build a **pipeline** of transformations that is:

- **Documented** -- each step carries a comment, its input/output
  variables, and its dependencies
- **Reproducible** -- the pipeline can be saved as a recipe and applied
  to new data
- **Shareable** -- recipes can be published to a public API where other
  researchers can discover and reuse them

The pipeline has three levels:

1. **Steps** -- individual transformations (compute, recode, rename,
   remove, join)
2. **Recipes** -- reusable collections of steps bundled with metadata
3. **Workflows** -- statistical estimations (`svymean`, `svytotal`,
   `svyby`) that produce the final tables

The package handles the survey design---stratification, clustering,
replicate weights---automatically through the `Survey` object. You focus
on the substantive analysis; metasurvey takes care of the plumbing.

## Installation

```{r install, eval = FALSE}
# Install from GitHub
# devtools::install_github("metaSurveyR/metasurvey")

library(metasurvey)
library(data.table)
```

## Creating a Survey Object

A `Survey` object bundles your microdata with metadata about weights, edition,
and survey type. We'll start with a simple example using simulated data that
mimics the structure of Uruguay's Encuesta Continua de Hogares (ECH).

The ECH is a rotating panel household survey conducted by the National Statistics
Institute (INE) of Uruguay. Key variables include:

- **e27**: Age in years
- **e26**: Sex (1 = Male, 2 = Female)
- **e51**: Educational level (1-14 coded scale)
- **POBPCOAC**: Labor force status
  - 2 = Employed
  - 3-5 = Unemployed
  - 6-8 = Inactive
- **ht11**: Household income
- **pesoano**: Annual expansion factor (weight)

```{r create-survey}
library(metasurvey)
library(data.table)

set.seed(42)
n <- 200

# Simulate ECH-like microdata
dt <- data.table(
  numero     = 1:n,                           # Household ID
  e27        = sample(18:80, n, replace = TRUE),     # Age
  e26        = sample(c(1, 2), n, replace = TRUE),   # Sex
  ht11       = round(runif(n, 5000, 80000)),  # Household income (Uruguayan pesos)
  POBPCOAC   = sample(c(2, 3, 4, 5, 6), n, replace = TRUE, 
                      prob = c(0.55, 0.03, 0.02, 0.03, 0.37)),  # Labor status
  dpto       = sample(1:19, n, replace = TRUE),      # Department
  pesoano    = round(runif(n, 0.5, 3.0), 4)   # Annual weight
)

# Create Survey object
svy <- Survey$new(
  data    = dt,
  edition = "2023",
  type    = "ech",
  psu     = NULL,              # No PSU for simple random sample
  engine  = "data.table",      # Fast data manipulation
  weight  = add_weight(annual = "pesoano")
)
```

The `add_weight()` function maps periodicity labels (e.g., "annual", "monthly")
to weight column names in your data. This allows the same recipe to work across
different survey editions.

You can inspect the data at any time:

```{r inspect}
head(get_data(svy), 3)
```

## Working with Steps

Steps are **lazy by default**: they are recorded but not applied until you call
`bake_steps()`. This allows you to:

1. Build a complete transformation pipeline
2. Inspect and validate steps before execution
3. Reuse step sequences as recipes
4. Ensure all dependencies exist before processing

### Computing New Variables

Use `step_compute()` to create derived variables. The package automatically:

- Validates that input variables exist
- Detects dependencies between steps
- Optimizes expressions for performance

```{r step-compute}
svy <- step_compute(svy,
  # Convert income to thousands for readability
  ht11_thousands = ht11 / 1000,
  
  # Create employment indicator following ILO definitions
  employed = ifelse(POBPCOAC == 2, 1, 0),
  
  # Working age population (14+ years, ECH standard)
  working_age = ifelse(e27 >= 14, 1, 0),
  
  comment = "Basic labor force indicators"
)
```

You can group calculations using the `.by` parameter (similar to `data.table`):

```{r step-compute-grouped}
# Calculate mean household income per department
svy <- step_compute(svy,
  mean_income_dept = mean(ht11, na.rm = TRUE),
  .by = "dpto",
  comment = "Department-level income averages"
)
```

### Recoding into Categories

Use `step_recode()` to create categorical variables from conditions.
Conditions are evaluated **top-to-bottom**, and the first match wins.

```{r step-recode}
# Recode labor force status (POBPCOAC) into meaningful categories
svy <- step_recode(svy, labor_status,
  POBPCOAC == 2      ~ "Employed",      # Ocupado
  POBPCOAC %in% 3:5  ~ "Unemployed",    # Desocupado
  POBPCOAC %in% 6:8  ~ "Inactive",      # Inactivo
  .default = "Not classified",
  comment = "Labor force status - ILO standard"
)

# Create standard age groups for labor statistics
svy <- step_recode(svy, age_group,
  e27 < 25  ~ "Youth (14-24)",
  e27 < 45  ~ "Adult (25-44)",
  e27 < 65  ~ "Mature (45-64)",
  .default = "Elderly (65+)",
  .to_factor = TRUE,          # Convert to factor
  ordered = TRUE,              # Ordered factor
  comment = "Age groups for labor analysis"
)

# Recode sex into descriptive labels
svy <- step_recode(svy, gender,
  e26 == 1 ~ "Male",
  e26 == 2 ~ "Female",
  .default = "Other",
  comment = "Gender classification"
)
```

### Renaming and Removing Variables

Rename variables for clarity or consistency:

```{r step-rename}
svy <- step_rename(svy, 
  age = e27,           # Rename e27 to age
  sex_code = e26       # Keep original as sex_code
)
```

Remove variables no longer needed:

```{r step-remove}
# Remove intermediate calculations
svy <- step_remove(svy, working_age, mean_income_dept)
```

### Joining External Data

Use `step_join()` to merge external reference data. This is useful for adding:

- Geographic names and classifications
- Exchange rates or deflators
- External benchmarks or targets

```{r step-join}
# Department names and regions
department_info <- data.table(
  dpto = 1:19,
  dpto_name = c("Montevideo", "Artigas", "Canelones", "Cerro Largo", 
                "Colonia", "Durazno", "Flores", "Florida", "Lavalleja",
                "Maldonado", "Paysandú", "Río Negro", "Rivera", "Rocha",
                "Salto", "San José", "Soriano", "Tacuarembó", "Treinta y Tres"),
  region = c("Montevideo", rep("Interior", 18))
)

svy <- step_join(svy, 
  department_info, 
  by = "dpto", 
  type = "left",
  comment = "Add department names and regions"
)
```

## Executing Transformations

### Baking Steps

Call `bake_steps()` to execute all pending transformations:

```{r bake, eval=FALSE}
svy <- bake_steps(svy)
head(get_data(svy), 3)
```

The step history is preserved for documentation and reproducibility:

```{r get-steps, eval=FALSE}
steps <- get_steps(svy)
length(steps)  # Number of transformation steps

# View step details
cat("Step 1:", steps[[1]]$name, "\n")
cat("Comment:", steps[[1]]$comments, "\n")
```

### Visualizing the Pipeline

You can visualize your transformation pipeline as a directed graph:

```{r view-graph, eval = FALSE}
view_graph(svy, init_step = "Load ECH 2023")
```

This creates an interactive graph showing:

- Data sources and joins
- Transformation steps
- Variable dependencies
- Comments and metadata

## Running Statistical EstimationsAfter preparing your data, use `workflow()` to compute survey estimates.
The function wraps `survey` package estimators [@lumley2004] and returns
tidy results with standard errors and coefficients of variation.

**Important:** Pass the survey object inside a `list()`.

### Basic Estimates

```{r workflow-mean}
# Estimate mean household income
result <- workflow(
  list(svy),
  survey::svymean(~ht11, na.rm = TRUE),
  estimation_type = "annual"
)

result
```

The output includes:

- `estimate`: Point estimate
- `se`: Standard error
- `cv`: Coefficient of variation
- `var_name`: Variable name
- `level`: Factor level (for categorical variables)

### Multiple Estimates

Compute several statistics in one call:

```{r workflow-multi}
results <- workflow(
  list(svy),
  survey::svymean(~ht11, na.rm = TRUE),          # Mean income
  survey::svytotal(~employed, na.rm = TRUE),      # Total employed
  survey::svymean(~labor_status, na.rm = TRUE),   # Employment distribution
  estimation_type = "annual"
)

results
```

### Domain Estimation

Compute estimates for subpopulations using `survey::svyby()`:

```{r workflow-domain}
# Mean income by gender
income_by_gender <- workflow(
  list(svy),
  survey::svyby(~ht11, ~gender, survey::svymean, na.rm = TRUE),
  estimation_type = "annual"
)

income_by_gender
```

## Quality Assessment

The **coefficient of variation (CV)** measures estimation reliability. Lower
CV indicates more precise estimates. Following INE Uruguay guidelines [@ine2023]:

| CV Range | Quality Category | Recommendation |
|----------|------------------|----------------|
| < 5%     | Excellent        | Use without restrictions |
| 5%–10%   | Very good        | Use with confidence |
| 10%–15%  | Good             | Use for most purposes |
| 15%–25%  | Acceptable       | Use with caution, note limitations |
| 25%–35%  | Poor             | Use only for general trends |
| ≥ 35%    | Unreliable       | Do not publish |

Use `evaluate_cv()` to classify estimate quality:

```{r cv}
# Check quality of mean income estimate
cv_percentage <- results$cv[1] * 100
quality <- evaluate_cv(cv_percentage)

cat("CV:", round(cv_percentage, 2), "%\n")
cat("Quality:", quality, "\n")
```

For official statistics, always report:

1. Point estimate
2. Standard error or confidence interval
3. Coefficient of variation
4. Quality classification
5. Sample size

## Working with Recipes

Recipes bundle transformation steps for **reproducibility** and **sharing**.
Once you've developed a working pipeline, convert it to a recipe that can be:

- Applied to different survey editions
- Shared with collaborators
- Published for transparency
- Version-controlled and documented

### Creating a Recipe

Create a recipe from your developed steps:

```{r recipe-create}
# Convert current steps to a recipe
labor_recipe <- steps_to_recipe(
  name        = "ECH Labor Force Indicators",
  user        = "National Statistics Office",
  svy         = svy,
  description = paste(
    "Standard labor force indicators following ILO definitions.",
    "Creates employment status, age groups, and gender classifications."
  ),
  steps       = get_steps(svy),
  topic       = "labor_statistics"
)

class(labor_recipe)
labor_recipe$name
```

Or define a recipe from scratch:

```{r recipe-define, eval=FALSE}
minimal_recipe <- recipe(
  name        = "Basic Demographics",
  user        = "analyst",
  svy         = survey_empty(type = "ech", edition = "2023"),
  description = "Basic demographic recoding",
  topic       = "demographics",
  
  # Define steps inline
  step_recode(
    gender,
    e26 == 1 ~ "Male",
    e26 == 2 ~ "Female",
    .default = "Other"
  ),
  
  step_recode(
    age_group,
    e27 < 18 ~ "Minor",
    e27 < 65 ~ "Adult",
    .default = "Senior"
  )
)
```

### Saving and Loading Recipes

Save recipes as JSON for sharing and version control:

```{r recipe-save}
tf <- tempfile(fileext = ".json")
save_recipe(labor_recipe, tf)

# Recipe saved with:
# - All step definitions
# - Metadata (author, description, topic)
# - Input/output variable documentation
# - Dependency information
```

Load recipes from disk:

```{r recipe-load, eval = FALSE}
loaded_recipe <- read_recipe("labor_force_indicators.json")

# Apply to a new survey
new_svy <- Survey$new(
  data = new_data,
  edition = "2024",
  type = "ech",
  psu = NULL,
  engine = "data.table",
  weight = add_weight(annual = "pesoano")
)

new_svy$add_recipe(loaded_recipe)
processed <- bake_recipes(new_svy)
```

### Recipe Documentation

Recipes automatically document their transformations:

```{r recipe-doc}
doc <- labor_recipe$doc()

# Examine documentation structure
names(doc)

# Input variables required
doc$input_variables

# Output variables created
doc$output_variables

# Pipeline summary
doc$pipeline  # Step-by-step transformation sequence
```

### Recipe Validation

Validate that a recipe is compatible with your data:

```{r recipe-validate, eval = FALSE}
# Check if all required variables exist
validation <- labor_recipe$validate(svy)

# Returns:
# - missing_vars: Variables not found in data
# - validation_passed: TRUE if all dependencies met
# - warnings: Potential issues
```

## Package Configuration

metasurvey provides global settings you can adjust for your workflow:

```{r config}
# Check current lazy-processing setting
lazy_default()  # TRUE = steps recorded but not executed immediately

# Check data-copy behavior  
use_copy_default()  # TRUE = operate on copies (safer but slower)

# View available computation engines
show_engines()  # "data.table", "dplyr", etc.
```

Change settings for your session:

```{r config-set, eval = FALSE}
# Disable lazy evaluation (execute steps immediately)
set_lazy(FALSE)

# Modify inplace (faster, but modifies original data)
set_use_copy(FALSE)

# Reset to defaults
set_lazy(TRUE)
set_use_copy(TRUE)
```

## Sharing your work: the recipe ecosystem

One of the goals of metasurvey is to reduce duplicated effort across the
community of survey researchers. If you have built a useful processing
pipeline, you can publish it so others can find and reuse it. The package
connects to a public API where recipes and workflows are stored:

```{r ecosystem, eval = FALSE}
# Browse existing recipes (no login required)
ech_recipes <- api_list_recipes(svy_type = "ech")
length(ech_recipes)

# Search for something specific
labor <- api_list_recipes(search = "labor")

# To publish your own, create an account first
api_register("Your Name", "you@example.com", "password")

# Then publish
api_publish_recipe(labor_recipe)
```

The ecosystem supports three levels of certification (community, reviewed,
official) and three account types (individual, institutional member,
institution). Institutional accounts require admin approval, which ensures
that certifications carry real weight.

For more details, see the guide
[The Recipe Ecosystem](api-ecosystem.html).

## Related packages

metasurvey is part of a growing ecosystem of R packages for household
survey analysis in Latin America:

- **[ech](https://calcita.github.io/ech/)** -- Functions for calculating
  socioeconomic indicators with Uruguay's ECH. Provides ready-made
  functions for poverty, income, education, and employment indicators.
- **[eph](https://docs.ropensci.org/eph/)** -- Tools for working with
  Argentina's Encuesta Permanente de Hogares. Published on rOpenSci.
  Covers data download, panel construction, and poverty calculation.
- **[survey](https://cran.r-project.org/package=survey)** -- The
  foundational package for design-based inference with complex surveys
  [@lumley2004]. metasurvey builds on top of it.

## Next Steps

Now that you understand the basics, explore advanced topics:

- **[ECH Case Study](ech-case-study.html)** -- Complete labor market analysis of Uruguay's household survey
- **[Panel Analysis](panel-analysis.html)** -- Rotative panels, PoolSurvey, longitudinal analysis
- **[The Recipe Ecosystem](api-ecosystem.html)** -- Publishing, discovering, and certifying recipes
- **[Recipes](recipes.html)** -- Advanced recipe workflows and sharing
- **[Complex Designs](complex-designs.html)** -- Stratification, clustering, replicate weights

## References

